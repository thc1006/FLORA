{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1ï½œè¯é‚¦å­¸ç¿’ç’°å¢ƒè¨­ç½®ï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆï¼‰\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================================================\n",
        "# â¶ å¾¹åº•æ¸…ç†ä¸¦é‡æ–°å®‰è£ç›¸å®¹å¥—ä»¶\n",
        "# =========================================================\n",
        "def install_compatible_federated_environment():\n",
        "    \"\"\"å¾¹åº•é‡æ–°å®‰è£ç›¸å®¹çš„è¯é‚¦å­¸ç¿’ç’°å¢ƒ\"\"\"\n",
        "    print(\"ğŸš€ å¾¹åº•é‡æ–°å®‰è£è¯é‚¦å­¸ç¿’ç’°å¢ƒ...\")\n",
        "\n",
        "    # æ­¥é©Ÿ1: å¾¹åº•å¸è¼‰ç›¸é—œå¥—ä»¶\n",
        "    packages_to_remove = [\n",
        "        \"tensorflow-federated\",\n",
        "        \"tensorflow-privacy\",\n",
        "        \"tensorflow-estimator\",\n",
        "        \"tensorflow-model-optimization\",\n",
        "        \"tensorflow\",\n",
        "        \"tf-keras\",\n",
        "        \"numpy\",\n",
        "        \"dp-accounting\"  # ä¹Ÿè¦ç§»é™¤é€™å€‹\n",
        "    ]\n",
        "\n",
        "    for pkg in packages_to_remove:\n",
        "        print(f\"ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ {pkg}...\")\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg\n",
        "        ], capture_output=True, check=False)\n",
        "\n",
        "    # æ­¥é©Ÿ2: æ¸…ç† pip å¿«å–\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"],\n",
        "                   capture_output=True, check=False)\n",
        "\n",
        "    # æ­¥é©Ÿ3: å…ˆå®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬\n",
        "    print(\"ğŸ“¦ å®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬...\")\n",
        "    numpy_install = subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "        \"numpy<2.0\"\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if numpy_install.returncode == 0:\n",
        "        print(\"âœ… NumPy<2.0 å®‰è£æˆåŠŸ\")\n",
        "    else:\n",
        "        print(f\"âŒ NumPy å®‰è£å¤±æ•—: {numpy_install.stderr}\")\n",
        "\n",
        "    # æ­¥é©Ÿ4: å®‰è£ dp-accountingï¼ˆTensorFlow Privacy çš„ä¾è³´ï¼‰\n",
        "    print(\"ğŸ“¦ å®‰è£ dp-accounting...\")\n",
        "    dp_install = subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "        \"dp-accounting==0.4.3\"  # æŒ‡å®šç›¸å®¹ç‰ˆæœ¬\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if dp_install.returncode == 0:\n",
        "        print(\"âœ… dp-accounting å®‰è£æˆåŠŸ\")\n",
        "    else:\n",
        "        print(f\"âŒ dp-accounting å®‰è£å¤±æ•—: {dp_install.stderr}\")\n",
        "\n",
        "    # æ­¥é©Ÿ5: å®‰è£èˆ‡ç•¶å‰ç’°å¢ƒç›¸å®¹çš„ç‰ˆæœ¬çµ„åˆ\n",
        "    print(\"ğŸ“¦ å®‰è£ç›¸å®¹çš„å¥—ä»¶çµ„åˆ...\")\n",
        "\n",
        "    # ä½¿ç”¨èˆ‡ Colab ç•¶å‰ç’°å¢ƒç›¸å®¹çš„ç‰ˆæœ¬\n",
        "    compatible_installs = [\n",
        "        \"tensorflow==2.15.0\",\n",
        "        \"tensorflow-estimator==2.15.0\",\n",
        "        \"tensorflow-privacy==0.9.0\",\n",
        "        \"tensorflow-federated==0.73.0\"\n",
        "    ]\n",
        "\n",
        "    for install_cmd in compatible_installs:\n",
        "        print(f\"ğŸ“¦ å®‰è£ {install_cmd}\")\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "            install_cmd\n",
        "        ], capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"âœ… {install_cmd} å®‰è£æˆåŠŸ\")\n",
        "        else:\n",
        "            print(f\"âŒ {install_cmd} å®‰è£å¤±æ•—\")\n",
        "            print(f\"   éŒ¯èª¤: {result.stderr}\")\n",
        "\n",
        "    # æ­¥é©Ÿ6: å®‰è£å…¶ä»–å¿…è¦çš„ä¾è³´\n",
        "    print(\"ğŸ“¦ å®‰è£å…¶ä»–å¿…è¦ä¾è³´...\")\n",
        "    other_deps = [\n",
        "        \"protobuf>=3.20,<4\",\n",
        "        \"absl-py>=1.0.0\",\n",
        "        \"attrs>=21.4.0\",\n",
        "        \"cachetools>=5.2\",\n",
        "        \"dm-tree>=0.1.8\",\n",
        "        \"grpcio>=1.48.2\",\n",
        "        \"jax>=0.4.1\",\n",
        "        \"jaxlib>=0.4.1\",\n",
        "        \"portpicker>=1.5.2\",\n",
        "        \"semantic-version>=2.10\",\n",
        "        \"sortedcontainers>=2.4.0\",\n",
        "        \"tqdm>=4.64.1\",\n",
        "        \"typing-extensions>=4.5.0\",\n",
        "        \"tensorflow-model-optimization>=0.7.3\",\n",
        "        \"tensorflow-compression>=2.13\",\n",
        "        \"scipy>=1.9.0\",\n",
        "        \"scikit-learn>=1.0.0\"\n",
        "    ]\n",
        "\n",
        "    for dep in other_deps:\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", dep\n",
        "        ], capture_output=True, check=False)\n",
        "\n",
        "# åŸ·è¡Œå®‰è£\n",
        "install_compatible_federated_environment()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âš ï¸  é‡è¦æé†’ï¼šå»ºè­°é‡æ–°å•Ÿå‹• Runtimeï¼\")\n",
        "print(\"   Runtime > Restart session\")\n",
        "print(\"   ç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤ Cell é€²è¡Œé©—è­‰\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# =========================================================\n",
        "# â· é©—è­‰å®‰è£çµæœ\n",
        "# =========================================================\n",
        "print(\"\\nğŸ” é©—è­‰å¥—ä»¶å®‰è£...\")\n",
        "\n",
        "# æ¸¬è©¦ dp_accounting\n",
        "try:\n",
        "    import dp_accounting\n",
        "    print(\"âœ… dp_accounting è¼‰å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ dp_accounting è¼‰å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# åŸºç¤æ¨¡çµ„\n",
        "try:\n",
        "    import json\n",
        "    import numpy as np\n",
        "    print(f\"âœ… NumPy {np.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "    import pandas as pd\n",
        "    from collections import OrderedDict\n",
        "    from typing import List, Tuple, Dict, Any, Optional\n",
        "    print(\"âœ… åŸºç¤æ¨¡çµ„å°å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åŸºç¤æ¨¡çµ„å°å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# TensorFlow é©—è­‰\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"âœ… TensorFlow {tf.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "\n",
        "    # è¨­å®š TensorFlow é…ç½®\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "    # GPU è¨­å®š\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"âœ… GPU å·²è¨­å®š: {len(gpus)} å€‹ GPU å¯ç”¨\")\n",
        "    else:\n",
        "        print(\"ğŸ“± ä½¿ç”¨ CPU æ¨¡å¼\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow è¼‰å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# TensorFlow Privacy é©—è­‰\n",
        "try:\n",
        "    import tensorflow_privacy as tfp\n",
        "    print(f\"âœ… TensorFlow Privacy {tfp.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow Privacy è¼‰å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# TensorFlow Federated é©—è­‰\n",
        "tff_loaded = False\n",
        "try:\n",
        "    import tensorflow_federated as tff\n",
        "    print(f\"âœ… TensorFlow Federated {tff.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "    tff_loaded = True\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow Federated è¼‰å…¥å¤±æ•—: {e}\")\n",
        "    print(\"ğŸ’¡ å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’æ ¸å¿ƒ\")\n",
        "\n",
        "# =========================================================\n",
        "# â¸ è¯é‚¦å­¸ç¿’é…ç½®\n",
        "# =========================================================\n",
        "FEDERATED_CONFIG = {\n",
        "    # åŸºæœ¬åƒæ•¸\n",
        "    \"num_clients\": 7,\n",
        "    \"base_stations\": [1, 2, 3, 4, 5, 6, 7],\n",
        "    \"rounds\": 30,\n",
        "    \"local_epochs\": 3,\n",
        "    \"batch_size\": 512,\n",
        "    \"learning_rate\": 0.001,\n",
        "\n",
        "    # é€²éšåŠŸèƒ½è¨­å®š\n",
        "    \"dp_enabled\": False,  # å¯ä»¥è¨­å®šç‚º True å¦‚æœ TF Privacy è¼‰å…¥æˆåŠŸ\n",
        "    \"secure_aggregation\": False,\n",
        "    \"compression_enabled\": False,\n",
        "    \"personalization\": False,\n",
        "\n",
        "    # å®¢æˆ¶ç«¯é¸æ“‡\n",
        "    \"clients_per_round\": 5,\n",
        "    \"min_available_clients\": 3,\n",
        "\n",
        "    # å·®åˆ†éš±ç§åƒæ•¸\n",
        "    \"dp_noise_multiplier\": 0.1,\n",
        "    \"dp_l2_norm_clip\": 1.0\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ—ï¸ è¯é‚¦å­¸ç¿’ç’°å¢ƒé…ç½®å®Œæˆ\")\n",
        "print(f\"ğŸ“Š é…ç½®: {FEDERATED_CONFIG['num_clients']} å®¢æˆ¶ç«¯, {FEDERATED_CONFIG['rounds']} è¼ªæ¬¡\")\n",
        "print(f\"ğŸ”’ éš±ç§ä¿è­·: {'å•Ÿç”¨' if FEDERATED_CONFIG['dp_enabled'] else 'é—œé–‰'}\")\n",
        "print(f\"ğŸŒ TensorFlow Federated: {'å¯ç”¨' if tff_loaded else 'ä½¿ç”¨è‡ªå¯¦ç¾ç‰ˆæœ¬'}\")\n",
        "\n",
        "# =========================================================\n",
        "# â¹ è‡ªå¯¦ç¾è¯é‚¦å­¸ç¿’æ ¸å¿ƒï¼ˆå‚™é¸æ–¹æ¡ˆï¼‰\n",
        "# =========================================================\n",
        "class SimpleFederatedLearning:\n",
        "    \"\"\"ç°¡åŒ–çš„è¯é‚¦å­¸ç¿’å¯¦ç¾ï¼Œé¿å…è¤‡é›œçš„å¥—ä»¶ç›¸ä¾æ€§\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.global_model_weights = None\n",
        "        self.training_history = {\n",
        "            \"rounds\": [],\n",
        "            \"avg_loss\": [],\n",
        "            \"client_losses\": [],\n",
        "            \"train_loss\": [],\n",
        "            \"test_loss\": []\n",
        "        }\n",
        "\n",
        "    def create_model(self, input_dim):\n",
        "        \"\"\"å‰µå»ºæ¨™æº–ç¥ç¶“ç¶²è·¯æ¨¡å‹\"\"\"\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "                tf.keras.layers.Dense(64, activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                tf.keras.layers.Dense(32, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.1),\n",
        "\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(self.config[\"learning_rate\"]),\n",
        "                loss='mse',\n",
        "                metrics=['mae']\n",
        "            )\n",
        "\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—: {e}\")\n",
        "            return None\n",
        "\n",
        "    def federated_averaging(self, client_weights_list, client_sizes):\n",
        "        \"\"\"å¯¦ç¾è¯é‚¦å¹³å‡ç®—æ³•\"\"\"\n",
        "        if not client_weights_list:\n",
        "            return None\n",
        "\n",
        "        # è¨ˆç®—åŠ æ¬Šå¹³å‡\n",
        "        total_size = sum(client_sizes)\n",
        "        averaged_weights = []\n",
        "\n",
        "        for layer_idx in range(len(client_weights_list[0])):\n",
        "            weighted_sum = None\n",
        "\n",
        "            for client_weights, size in zip(client_weights_list, client_sizes):\n",
        "                layer_weight = client_weights[layer_idx] * (size / total_size)\n",
        "\n",
        "                if weighted_sum is None:\n",
        "                    weighted_sum = layer_weight\n",
        "                else:\n",
        "                    weighted_sum += layer_weight\n",
        "\n",
        "            averaged_weights.append(weighted_sum)\n",
        "\n",
        "        return averaged_weights\n",
        "\n",
        "    def train_client(self, client_id, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"è¨“ç·´å–®ä¸€å®¢æˆ¶ç«¯\"\"\"\n",
        "        # å‰µå»ºæœ¬åœ°æ¨¡å‹\n",
        "        model = self.create_model(X_train.shape[1])\n",
        "\n",
        "        if model is None:\n",
        "            return None, 0, {}\n",
        "\n",
        "        # å¦‚æœæœ‰å…¨åŸŸæ¨¡å‹æ¬Šé‡ï¼Œå‰‡è¼‰å…¥\n",
        "        if self.global_model_weights is not None:\n",
        "            model.set_weights(self.global_model_weights)\n",
        "\n",
        "        # æœ¬åœ°è¨“ç·´\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.config[\"local_epochs\"],\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        return model.get_weights(), len(X_train), history.history\n",
        "\n",
        "print(\"âœ… ç°¡åŒ–è¯é‚¦å­¸ç¿’é¡åˆ¥å®šç¾©å®Œæˆ\")\n",
        "\n",
        "# å‰µå»ºè¯é‚¦å­¸ç¿’ç³»çµ±\n",
        "federated_system = SimpleFederatedLearning(FEDERATED_CONFIG)\n",
        "\n",
        "# =========================================================\n",
        "# âº å¦‚æœ TFF è¼‰å…¥æˆåŠŸï¼Œå‰µå»º TFF ç‰ˆæœ¬\n",
        "# =========================================================\n",
        "if tff_loaded:\n",
        "    print(\"\\nğŸ¯ TensorFlow Federated è¼‰å…¥æˆåŠŸï¼Œæº–å‚™ TFF åŠŸèƒ½...\")\n",
        "\n",
        "    def create_tff_model():\n",
        "        \"\"\"å‰µå»º TFF ç›¸å®¹çš„æ¨¡å‹\"\"\"\n",
        "        return SimpleFederatedLearning(FEDERATED_CONFIG).create_model\n",
        "\n",
        "    print(\"âœ… TFF æ¨¡å‹å·¥å» å‡½æ•¸å·²æº–å‚™\")\n",
        "else:\n",
        "    print(\"\\nğŸ“Œ å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’ç³»çµ±\")\n",
        "\n",
        "print(\"\\nğŸ‰ è¯é‚¦å­¸ç¿’ç’°å¢ƒæº–å‚™å®Œæˆï¼\")\n",
        "print(\"ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ Cell 2 é€²è¡Œè³‡æ–™æº–å‚™\")\n",
        "\n",
        "# æœ€çµ‚ç’°å¢ƒæª¢æŸ¥\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š æœ€çµ‚ç’°å¢ƒç‹€æ…‹æª¢æŸ¥ï¼š\")\n",
        "modules_status = {\n",
        "    \"NumPy\": False,\n",
        "    \"TensorFlow\": False,\n",
        "    \"TensorFlow Federated\": tff_loaded,\n",
        "    \"TensorFlow Privacy\": False,\n",
        "    \"dp_accounting\": False\n",
        "}\n",
        "\n",
        "try:\n",
        "    import numpy\n",
        "    modules_status[\"NumPy\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import tensorflow\n",
        "    modules_status[\"TensorFlow\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import tensorflow_privacy\n",
        "    modules_status[\"TensorFlow Privacy\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import dp_accounting\n",
        "    modules_status[\"dp_accounting\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "for module, status in modules_status.items():\n",
        "    print(f\"  {module}: {'âœ… å¯ç”¨' if status else 'âŒ ä¸å¯ç”¨'}\")\n",
        "\n",
        "if all(modules_status.values()):\n",
        "    print(\"\\nğŸš€ æ‰€æœ‰æ¨¡çµ„è¼‰å…¥æˆåŠŸï¼å¯ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  éƒ¨åˆ†æ¨¡çµ„æœªè¼‰å…¥ï¼Œä½†å¯ä»¥ä½¿ç”¨åŸºæœ¬åŠŸèƒ½\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "0_7Uz5LHMpjh",
        "outputId": "7f91f755-53df-4338-d3f5-d0b21d947245"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¾¹åº•é‡æ–°å®‰è£è¯é‚¦å­¸ç¿’ç’°å¢ƒ...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-federated...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-privacy...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-estimator...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-model-optimization...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tf-keras...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ numpy...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ dp-accounting...\n",
            "ğŸ“¦ å®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬...\n",
            "âœ… NumPy<2.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ dp-accounting...\n",
            "âœ… dp-accounting å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ç›¸å®¹çš„å¥—ä»¶çµ„åˆ...\n",
            "ğŸ“¦ å®‰è£ tensorflow==2.15.0\n",
            "âœ… tensorflow==2.15.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-estimator==2.15.0\n",
            "âœ… tensorflow-estimator==2.15.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-privacy==0.9.0\n",
            "âœ… tensorflow-privacy==0.9.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-federated==0.73.0\n",
            "âœ… tensorflow-federated==0.73.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£å…¶ä»–å¿…è¦ä¾è³´...\n",
            "\n",
            "============================================================\n",
            "âš ï¸  é‡è¦æé†’ï¼šå»ºè­°é‡æ–°å•Ÿå‹• Runtimeï¼\n",
            "   Runtime > Restart session\n",
            "   ç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤ Cell é€²è¡Œé©—è­‰\n",
            "============================================================\n",
            "\n",
            "ğŸ” é©—è­‰å¥—ä»¶å®‰è£...\n",
            "âœ… dp_accounting è¼‰å…¥æˆåŠŸ\n",
            "âœ… NumPy 1.25.2 è¼‰å…¥æˆåŠŸ\n",
            "âœ… åŸºç¤æ¨¡çµ„å°å…¥æˆåŠŸ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… TensorFlow 2.14.1 è¼‰å…¥æˆåŠŸ\n",
            "ğŸ“± ä½¿ç”¨ CPU æ¨¡å¼\n",
            "âœ… TensorFlow Privacy 0.9.0 è¼‰å…¥æˆåŠŸ\n",
            "âœ… TensorFlow Federated 0.73.0 è¼‰å…¥æˆåŠŸ\n",
            "\n",
            "ğŸ—ï¸ è¯é‚¦å­¸ç¿’ç’°å¢ƒé…ç½®å®Œæˆ\n",
            "ğŸ“Š é…ç½®: 7 å®¢æˆ¶ç«¯, 30 è¼ªæ¬¡\n",
            "ğŸ”’ éš±ç§ä¿è­·: é—œé–‰\n",
            "ğŸŒ TensorFlow Federated: å¯ç”¨\n",
            "âœ… ç°¡åŒ–è¯é‚¦å­¸ç¿’é¡åˆ¥å®šç¾©å®Œæˆ\n",
            "\n",
            "ğŸ¯ TensorFlow Federated è¼‰å…¥æˆåŠŸï¼Œæº–å‚™ TFF åŠŸèƒ½...\n",
            "âœ… TFF æ¨¡å‹å·¥å» å‡½æ•¸å·²æº–å‚™\n",
            "\n",
            "ğŸ‰ è¯é‚¦å­¸ç¿’ç’°å¢ƒæº–å‚™å®Œæˆï¼\n",
            "ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ Cell 2 é€²è¡Œè³‡æ–™æº–å‚™\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š æœ€çµ‚ç’°å¢ƒç‹€æ…‹æª¢æŸ¥ï¼š\n",
            "  NumPy: âœ… å¯ç”¨\n",
            "  TensorFlow: âœ… å¯ç”¨\n",
            "  TensorFlow Federated: âœ… å¯ç”¨\n",
            "  TensorFlow Privacy: âœ… å¯ç”¨\n",
            "  dp_accounting: âœ… å¯ç”¨\n",
            "\n",
            "ğŸš€ æ‰€æœ‰æ¨¡çµ„è¼‰å…¥æˆåŠŸï¼å¯ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgxli4SfrJQ3",
        "outputId": "54c67ba2-154f-4b7c-ed39-0251b4c0a66a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¾¹åº•é‡æ–°å®‰è£è¯é‚¦å­¸ç¿’ç’°å¢ƒ...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-federated...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-privacy...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-estimator...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow-model-optimization...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tensorflow...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ tf-keras...\n",
            "ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ numpy...\n",
            "ğŸ“¦ å®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬...\n",
            "âœ… NumPy<2.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ç›¸å®¹çš„å¥—ä»¶çµ„åˆ...\n",
            "ğŸ“¦ å®‰è£ tensorflow==2.15.0\n",
            "âœ… tensorflow==2.15.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-federated==0.73.0\n",
            "âœ… tensorflow-federated==0.73.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-privacy==0.9.0\n",
            "âœ… tensorflow-privacy==0.9.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£ tensorflow-estimator==2.15.0\n",
            "âœ… tensorflow-estimator==2.15.0 å®‰è£æˆåŠŸ\n",
            "ğŸ“¦ å®‰è£å…¶ä»–å¿…è¦ä¾è³´...\n",
            "\n",
            "============================================================\n",
            "âš ï¸  é‡è¦æé†’ï¼šè«‹ç«‹å³é‡æ–°å•Ÿå‹• Runtimeï¼\n",
            "   Runtime > Restart session\n",
            "   ç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤ Cell é€²è¡Œé©—è­‰\n",
            "============================================================\n",
            "\n",
            "ğŸ” é©—è­‰å¥—ä»¶å®‰è£...\n",
            "âœ… NumPy 1.26.4 è¼‰å…¥æˆåŠŸ\n",
            "âœ… åŸºç¤æ¨¡çµ„å°å…¥æˆåŠŸ\n",
            "âœ… TensorFlow 2.15.0 è¼‰å…¥æˆåŠŸ\n",
            "ğŸ“± ä½¿ç”¨ CPU æ¨¡å¼\n",
            "âŒ TensorFlow Federated è¼‰å…¥å¤±æ•—: No module named 'dp_accounting'\n",
            "ğŸ’¡ å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’æ ¸å¿ƒ\n",
            "âŒ TensorFlow Privacy è¼‰å…¥å¤±æ•—: No module named 'dp_accounting'\n",
            "\n",
            "ğŸ—ï¸ è¯é‚¦å­¸ç¿’ç’°å¢ƒé…ç½®å®Œæˆ\n",
            "ğŸ“Š é…ç½®: 7 å®¢æˆ¶ç«¯, 30 è¼ªæ¬¡\n",
            "ğŸ”’ éš±ç§ä¿è­·: é—œé–‰\n",
            "âœ… ç°¡åŒ–è¯é‚¦å­¸ç¿’é¡åˆ¥å®šç¾©å®Œæˆ\n",
            "\n",
            "ğŸ‰ è¯é‚¦å­¸ç¿’ç’°å¢ƒæº–å‚™å®Œæˆï¼\n",
            "ğŸ’¡ å¦‚æœ TensorFlow Federated è¼‰å…¥å¤±æ•—ï¼Œç³»çµ±å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’æ ¸å¿ƒ\n",
            "ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ Cell 2 é€²è¡Œè³‡æ–™æº–å‚™\n",
            "\n",
            "âœ… ç’°å¢ƒæ¸¬è©¦é€šéï¼Œå¯ä»¥ç¹¼çºŒåŸ·è¡Œ\n",
            "\n",
            "ğŸš€ å¯ä»¥ç›´æ¥åŸ·è¡Œ Cell 2ï¼Œç„¡éœ€é‡å•Ÿ Runtime\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 1ï½œè¯é‚¦å­¸ç¿’ç’°å¢ƒè¨­ç½®ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================================================\n",
        "# â¶ å¾¹åº•æ¸…ç†ä¸¦é‡æ–°å®‰è£ç›¸å®¹å¥—ä»¶\n",
        "# =========================================================\n",
        "def install_compatible_federated_environment():\n",
        "    \"\"\"å¾¹åº•é‡æ–°å®‰è£ç›¸å®¹çš„è¯é‚¦å­¸ç¿’ç’°å¢ƒ\"\"\"\n",
        "    print(\"ğŸš€ å¾¹åº•é‡æ–°å®‰è£è¯é‚¦å­¸ç¿’ç’°å¢ƒ...\")\n",
        "\n",
        "    # æ­¥é©Ÿ1: å¾¹åº•å¸è¼‰ç›¸é—œå¥—ä»¶\n",
        "    packages_to_remove = [\n",
        "        \"tensorflow-federated\",\n",
        "        \"tensorflow-privacy\",\n",
        "        \"tensorflow-estimator\",\n",
        "        \"tensorflow-model-optimization\",\n",
        "        \"tensorflow\",\n",
        "        \"tf-keras\",\n",
        "        \"numpy\"  # ä¹Ÿè¦é‡æ–°å®‰è£ NumPy\n",
        "    ]\n",
        "\n",
        "    for pkg in packages_to_remove:\n",
        "        print(f\"ğŸ—‘ï¸ å¾¹åº•ç§»é™¤ {pkg}...\")\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg\n",
        "        ], capture_output=True, check=False)\n",
        "\n",
        "    # æ­¥é©Ÿ2: æ¸…ç† pip å¿«å–\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"],\n",
        "                   capture_output=True, check=False)\n",
        "\n",
        "    # æ­¥é©Ÿ3: å…ˆå®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬\n",
        "    print(\"ğŸ“¦ å®‰è£ç›¸å®¹çš„ NumPy ç‰ˆæœ¬...\")\n",
        "    numpy_install = subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "        \"numpy<2.0\"  # å®‰è£ NumPy 1.x ç‰ˆæœ¬\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if numpy_install.returncode == 0:\n",
        "        print(\"âœ… NumPy<2.0 å®‰è£æˆåŠŸ\")\n",
        "    else:\n",
        "        print(f\"âŒ NumPy å®‰è£å¤±æ•—: {numpy_install.stderr}\")\n",
        "\n",
        "    # æ­¥é©Ÿ4: å®‰è£èˆ‡ç•¶å‰ç’°å¢ƒç›¸å®¹çš„ç‰ˆæœ¬çµ„åˆ\n",
        "    print(\"ğŸ“¦ å®‰è£ç›¸å®¹çš„å¥—ä»¶çµ„åˆ...\")\n",
        "\n",
        "    # ä½¿ç”¨èˆ‡ Colab ç•¶å‰ç’°å¢ƒç›¸å®¹çš„ç‰ˆæœ¬\n",
        "    compatible_installs = [\n",
        "        \"tensorflow==2.15.0\",  # ä½¿ç”¨è¼ƒæ–°ä½†ç©©å®šçš„ç‰ˆæœ¬\n",
        "        \"tensorflow-federated==0.73.0\",  # èˆ‡ TF 2.15 ç›¸å®¹\n",
        "        \"tensorflow-privacy==0.9.0\",  # æœ€æ–°ç›¸å®¹ç‰ˆæœ¬\n",
        "        \"tensorflow-estimator==2.15.0\",  # æ˜ç¢ºæŒ‡å®š estimator ç‰ˆæœ¬\n",
        "    ]\n",
        "\n",
        "    for install_cmd in compatible_installs:\n",
        "        print(f\"ğŸ“¦ å®‰è£ {install_cmd}\")\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "            \"--no-deps\", install_cmd  # ä½¿ç”¨ --no-deps é¿å…è‡ªå‹•å‡ç´šä¾è³´\n",
        "        ], capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"âœ… {install_cmd} å®‰è£æˆåŠŸ\")\n",
        "        else:\n",
        "            print(f\"âŒ {install_cmd} å®‰è£å¤±æ•—\")\n",
        "            print(f\"   éŒ¯èª¤: {result.stderr}\")\n",
        "\n",
        "    # æ­¥é©Ÿ5: å®‰è£å…¶ä»–å¿…è¦çš„ä¾è³´\n",
        "    print(\"ğŸ“¦ å®‰è£å…¶ä»–å¿…è¦ä¾è³´...\")\n",
        "    other_deps = [\n",
        "        \"protobuf>=3.20,<4\",\n",
        "        \"absl-py>=1.0.0\",\n",
        "        \"attrs>=21.4.0\",\n",
        "        \"cachetools>=5.2\",\n",
        "        \"dm-tree>=0.1.8\",\n",
        "        \"grpcio>=1.48.2\",\n",
        "        \"jax>=0.4.1\",\n",
        "        \"jaxlib>=0.4.1\",\n",
        "        \"portpicker>=1.5.2\",\n",
        "        \"semantic-version>=2.10\",\n",
        "        \"sortedcontainers>=2.4.0\",\n",
        "        \"tqdm>=4.64.1\",\n",
        "        \"typing-extensions>=4.5.0\"\n",
        "    ]\n",
        "\n",
        "    for dep in other_deps:\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", dep\n",
        "        ], capture_output=True, check=False)\n",
        "\n",
        "# åŸ·è¡Œå®‰è£\n",
        "install_compatible_federated_environment()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âš ï¸  é‡è¦æé†’ï¼šè«‹ç«‹å³é‡æ–°å•Ÿå‹• Runtimeï¼\")\n",
        "print(\"   Runtime > Restart session\")\n",
        "print(\"   ç„¶å¾Œé‡æ–°åŸ·è¡Œæ­¤ Cell é€²è¡Œé©—è­‰\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# =========================================================\n",
        "# â· é©—è­‰å®‰è£çµæœ\n",
        "# =========================================================\n",
        "print(\"\\nğŸ” é©—è­‰å¥—ä»¶å®‰è£...\")\n",
        "\n",
        "# åŸºç¤æ¨¡çµ„\n",
        "try:\n",
        "    import json\n",
        "    import numpy as np\n",
        "    print(f\"âœ… NumPy {np.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "    import pandas as pd\n",
        "    from collections import OrderedDict\n",
        "    from typing import List, Tuple, Dict, Any, Optional\n",
        "    print(\"âœ… åŸºç¤æ¨¡çµ„å°å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åŸºç¤æ¨¡çµ„å°å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# TensorFlow é©—è­‰\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"âœ… TensorFlow {tf.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "\n",
        "    # è¨­å®š TensorFlow é…ç½®\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "    # GPU è¨­å®š\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"âœ… GPU å·²è¨­å®š: {len(gpus)} å€‹ GPU å¯ç”¨\")\n",
        "    else:\n",
        "        print(\"ğŸ“± ä½¿ç”¨ CPU æ¨¡å¼\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow è¼‰å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# TensorFlow Federated é©—è­‰\n",
        "try:\n",
        "    import tensorflow_federated as tff\n",
        "    print(f\"âœ… TensorFlow Federated {tff.__version__} è¼‰å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow Federated è¼‰å…¥å¤±æ•—: {e}\")\n",
        "    print(\"ğŸ’¡ å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’æ ¸å¿ƒ\")\n",
        "\n",
        "# TensorFlow Privacy é©—è­‰\n",
        "try:\n",
        "    import tensorflow_privacy as tfp\n",
        "    print(f\"âœ… TensorFlow Privacy è¼‰å…¥æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ TensorFlow Privacy è¼‰å…¥å¤±æ•—: {e}\")\n",
        "\n",
        "# =========================================================\n",
        "# â¸ è¯é‚¦å­¸ç¿’é…ç½®ï¼ˆç°¡åŒ–ç‰ˆï¼Œç¢ºä¿ç©©å®šæ€§ï¼‰\n",
        "# =========================================================\n",
        "FEDERATED_CONFIG = {\n",
        "    # åŸºæœ¬åƒæ•¸\n",
        "    \"num_clients\": 7,\n",
        "    \"base_stations\": [1, 2, 3, 4, 5, 6, 7],\n",
        "    \"rounds\": 30,  # æ¸›å°‘è¼ªæ•¸ç¢ºä¿ç©©å®šæ€§\n",
        "    \"local_epochs\": 3,\n",
        "    \"batch_size\": 512,\n",
        "    \"learning_rate\": 0.001,\n",
        "\n",
        "    # ç°¡åŒ–è¨­å®šé¿å…ç›¸ä¾æ€§å•é¡Œ\n",
        "    \"dp_enabled\": False,  # æš«æ™‚é—œé–‰å·®åˆ†éš±ç§\n",
        "    \"secure_aggregation\": False,\n",
        "    \"compression_enabled\": False,\n",
        "    \"personalization\": False,\n",
        "\n",
        "    # å®¢æˆ¶ç«¯é¸æ“‡\n",
        "    \"clients_per_round\": 5,\n",
        "    \"min_available_clients\": 3,\n",
        "\n",
        "    # å·®åˆ†éš±ç§åƒæ•¸ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
        "    \"dp_noise_multiplier\": 0.1,\n",
        "    \"dp_l2_norm_clip\": 1.0\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ—ï¸ è¯é‚¦å­¸ç¿’ç’°å¢ƒé…ç½®å®Œæˆ\")\n",
        "print(f\"ğŸ“Š é…ç½®: {FEDERATED_CONFIG['num_clients']} å®¢æˆ¶ç«¯, {FEDERATED_CONFIG['rounds']} è¼ªæ¬¡\")\n",
        "print(f\"ğŸ”’ éš±ç§ä¿è­·: {'å•Ÿç”¨' if FEDERATED_CONFIG['dp_enabled'] else 'é—œé–‰'}\")\n",
        "\n",
        "# =========================================================\n",
        "# â¹ è‡ªå¯¦ç¾è¯é‚¦å­¸ç¿’æ ¸å¿ƒï¼ˆå‚™é¸æ–¹æ¡ˆï¼‰\n",
        "# =========================================================\n",
        "class SimpleFederatedLearning:\n",
        "    \"\"\"ç°¡åŒ–çš„è¯é‚¦å­¸ç¿’å¯¦ç¾ï¼Œé¿å…è¤‡é›œçš„å¥—ä»¶ç›¸ä¾æ€§\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.global_model_weights = None\n",
        "        self.training_history = {\n",
        "            \"rounds\": [],\n",
        "            \"avg_loss\": [],\n",
        "            \"client_losses\": []\n",
        "        }\n",
        "\n",
        "    def create_model(self, input_dim):\n",
        "        \"\"\"å‰µå»ºæ¨™æº–ç¥ç¶“ç¶²è·¯æ¨¡å‹\"\"\"\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "                tf.keras.layers.Dense(64, activation='relu'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                tf.keras.layers.Dense(32, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.1),\n",
        "\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(self.config[\"learning_rate\"]),\n",
        "                loss='mse',\n",
        "                metrics=['mae']\n",
        "            )\n",
        "\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—: {e}\")\n",
        "            return None\n",
        "\n",
        "    def federated_averaging(self, client_weights_list, client_sizes):\n",
        "        \"\"\"å¯¦ç¾è¯é‚¦å¹³å‡ç®—æ³•\"\"\"\n",
        "        if not client_weights_list:\n",
        "            return None\n",
        "\n",
        "        # è¨ˆç®—åŠ æ¬Šå¹³å‡\n",
        "        total_size = sum(client_sizes)\n",
        "        averaged_weights = []\n",
        "\n",
        "        for layer_idx in range(len(client_weights_list[0])):\n",
        "            weighted_sum = None\n",
        "\n",
        "            for client_weights, size in zip(client_weights_list, client_sizes):\n",
        "                layer_weight = client_weights[layer_idx] * (size / total_size)\n",
        "\n",
        "                if weighted_sum is None:\n",
        "                    weighted_sum = layer_weight\n",
        "                else:\n",
        "                    weighted_sum += layer_weight\n",
        "\n",
        "            averaged_weights.append(weighted_sum)\n",
        "\n",
        "        return averaged_weights\n",
        "\n",
        "    def train_client(self, client_id, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"è¨“ç·´å–®ä¸€å®¢æˆ¶ç«¯\"\"\"\n",
        "        # å‰µå»ºæœ¬åœ°æ¨¡å‹\n",
        "        model = self.create_model(X_train.shape[1])\n",
        "\n",
        "        if model is None:\n",
        "            return None, 0, {}\n",
        "\n",
        "        # å¦‚æœæœ‰å…¨åŸŸæ¨¡å‹æ¬Šé‡ï¼Œå‰‡è¼‰å…¥\n",
        "        if self.global_model_weights is not None:\n",
        "            model.set_weights(self.global_model_weights)\n",
        "\n",
        "        # æœ¬åœ°è¨“ç·´\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.config[\"local_epochs\"],\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        return model.get_weights(), len(X_train), history.history\n",
        "\n",
        "print(\"âœ… ç°¡åŒ–è¯é‚¦å­¸ç¿’é¡åˆ¥å®šç¾©å®Œæˆ\")\n",
        "\n",
        "# å‰µå»ºè¯é‚¦å­¸ç¿’ç³»çµ±\n",
        "federated_system = SimpleFederatedLearning(FEDERATED_CONFIG)\n",
        "\n",
        "print(\"\\nğŸ‰ è¯é‚¦å­¸ç¿’ç’°å¢ƒæº–å‚™å®Œæˆï¼\")\n",
        "print(\"ğŸ’¡ å¦‚æœ TensorFlow Federated è¼‰å…¥å¤±æ•—ï¼Œç³»çµ±å°‡ä½¿ç”¨è‡ªå¯¦ç¾çš„è¯é‚¦å­¸ç¿’æ ¸å¿ƒ\")\n",
        "print(\"ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ Cell 2 é€²è¡Œè³‡æ–™æº–å‚™\")\n",
        "\n",
        "# æª¢æŸ¥æ˜¯å¦éœ€è¦é‡å•Ÿ\n",
        "need_restart = False\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    # æ¸¬è©¦æ˜¯å¦å¯ä»¥æ­£å¸¸ä½¿ç”¨\n",
        "    test_array = np.array([1, 2, 3])\n",
        "    test_tensor = tf.constant(test_array)\n",
        "    print(\"\\nâœ… ç’°å¢ƒæ¸¬è©¦é€šéï¼Œå¯ä»¥ç¹¼çºŒåŸ·è¡Œ\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ ç’°å¢ƒæ¸¬è©¦å¤±æ•—: {e}\")\n",
        "    print(\"âš ï¸  è«‹é‡æ–°å•Ÿå‹• Runtime å¾Œå†æ¬¡åŸ·è¡Œæ­¤ Cell\")\n",
        "    need_restart = True\n",
        "\n",
        "if not need_restart:\n",
        "    print(\"\\nğŸš€ å¯ä»¥ç›´æ¥åŸ·è¡Œ Cell 2ï¼Œç„¡éœ€é‡å•Ÿ Runtime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "i2VCtWa1rbmT",
        "outputId": "991062e6-0929-4116-e564-096e03615654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” è¼‰å…¥å®Œæ•´è³‡æ–™é›†...\n",
            "âœ… è¼‰å…¥ 35,512,393 ç­†è¨˜éŒ„\n",
            "ğŸ“Š å®¢æˆ¶ç«¯ 0 (BS-1): 4,039,936 è¨“ç·´æ¨£æœ¬, 1,009,985 æ¸¬è©¦æ¨£æœ¬\n",
            "ğŸ“Š å®¢æˆ¶ç«¯ 1 (BS-2): 4,057,276 è¨“ç·´æ¨£æœ¬, 1,014,319 æ¸¬è©¦æ¨£æœ¬\n",
            "ğŸ“Š å®¢æˆ¶ç«¯ 2 (BS-3): 4,130,032 è¨“ç·´æ¨£æœ¬, 1,032,509 æ¸¬è©¦æ¨£æœ¬\n",
            "ğŸ“Š å®¢æˆ¶ç«¯ 3 (BS-4): 3,943,985 è¨“ç·´æ¨£æœ¬, 985,997 æ¸¬è©¦æ¨£æœ¬\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1163851579.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# åŸ·è¡Œè³‡æ–™è™•ç†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFederatedDataProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEDERATED_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mclient_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mfederated_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tf_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-1163851579.py\u001b[0m in \u001b[0;36mload_and_split_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# æœ¬åœ°æ¨™æº–åŒ–\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 self.mean_, self.var_, self.n_samples_seen_ = _incremental_mean_and_var(\n\u001b[0m\u001b[1;32m   1017\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_incremental_mean_and_var\u001b[0;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mnew_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0mnew_sample_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nan_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0mupdated_sample_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_sample_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_sample_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2314\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Cell 2 | è¯é‚¦è³‡æ–™åˆ†å‰²èˆ‡é è™•ç†\n",
        "import hashlib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class FederatedDataProcessor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.client_scalers = {}\n",
        "        self.global_stats = {}\n",
        "\n",
        "    def load_and_split_data(self):\n",
        "        \"\"\"è¼‰å…¥è³‡æ–™ä¸¦æŒ‰åŸºç«™åˆ†å‰²ç‚ºè¯é‚¦å®¢æˆ¶ç«¯\"\"\"\n",
        "        print(\"ğŸ” è¼‰å…¥å®Œæ•´è³‡æ–™é›†...\")\n",
        "\n",
        "        # è¼‰å…¥è™•ç†å¾Œçš„ç‰¹å¾µè³‡æ–™\n",
        "        if os.path.exists(\"coloran_processed_features.parquet\"):\n",
        "            df = pd.read_parquet(\"coloran_processed_features.parquet\")\n",
        "            print(f\"âœ… è¼‰å…¥ {len(df):,} ç­†è¨˜éŒ„\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"è«‹å…ˆåŸ·è¡ŒåŸå§‹ Cell 3 ç”Ÿæˆç‰¹å¾µè³‡æ–™\")\n",
        "\n",
        "        # è¼‰å…¥ç‰¹å¾µåç¨±\n",
        "        if not os.path.exists(\"feature_metadata.json\"):\n",
        "             raise FileNotFoundError(\"è«‹ç¢ºèª feature_metadata.json æª”æ¡ˆå­˜åœ¨\")\n",
        "        with open(\"feature_metadata.json\", \"r\") as f:\n",
        "            feature_names = json.load(f)[\"feature_names\"]\n",
        "\n",
        "        # æŒ‰åŸºç«™åˆ†å‰²è³‡æ–™\n",
        "        client_data = {}\n",
        "        data_stats = {}\n",
        "\n",
        "        for i, bs_id in enumerate(self.config[\"base_stations\"]):\n",
        "            # ç¢ºä¿ bs_id æ˜¯æ•´æ•¸å‹åˆ¥ä»¥ä¾¿æ¯”è¼ƒ\n",
        "            df['bs_id'] = df['bs_id'].astype(int)\n",
        "            client_df = df[df[\"bs_id\"] == bs_id].copy()\n",
        "\n",
        "            if len(client_df) == 0:\n",
        "                print(f\"âš ï¸ åŸºç«™ {bs_id} ç„¡è³‡æ–™ï¼Œè·³é\")\n",
        "                continue\n",
        "\n",
        "            # æå–ç‰¹å¾µå’Œç›®æ¨™\n",
        "            X = client_df[feature_names].astype(np.float32).values\n",
        "            y = client_df[\"allocation_efficiency\"].astype(np.float32).values\n",
        "\n",
        "            # æ¸…ç†è³‡æ–™\n",
        "            mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
        "            X, y = X[mask], y[mask]\n",
        "\n",
        "            # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # æœ¬åœ°æ¨™æº–åŒ–\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
        "            X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "            client_data[f\"client_{i}\"] = {\n",
        "                \"bs_id\": bs_id,\n",
        "                \"X_train\": X_train_scaled, \"y_train\": y_train,\n",
        "                \"X_test\": X_test_scaled, \"y_test\": y_test,\n",
        "                \"scaler\": scaler, \"feature_names\": feature_names\n",
        "            }\n",
        "\n",
        "            data_stats[f\"client_{i}\"] = {\"bs_id\": bs_id, \"train_samples\": len(X_train), \"test_samples\": len(X_test)}\n",
        "            print(f\"ğŸ“Š å®¢æˆ¶ç«¯ {i} (BS-{bs_id}): {len(X_train):,} è¨“ç·´æ¨£æœ¬, {len(X_test):,} æ¸¬è©¦æ¨£æœ¬\")\n",
        "\n",
        "        self.global_stats = {\"total_clients\": len(client_data), \"client_stats\": data_stats}\n",
        "        print(f\"\\nğŸŒ è¯é‚¦è³‡æ–™åˆ†å‰²å®Œæˆï¼Œå…± {self.global_stats['total_clients']} å€‹æœ‰æ•ˆå®¢æˆ¶ç«¯ã€‚\")\n",
        "\n",
        "        return client_data, feature_names\n",
        "\n",
        "    def create_tf_datasets(self, client_data):\n",
        "        \"\"\"å‰µå»º TensorFlow Federated è³‡æ–™é›†\"\"\"\n",
        "        federated_train_data, federated_test_data = [], []\n",
        "        for client_id, data in client_data.items():\n",
        "            train_ds = tf.data.Dataset.from_tensor_slices({\"x\": data[\"X_train\"], \"y\": data[\"y_train\"]}).batch(self.config[\"batch_size\"])\n",
        "            test_ds = tf.data.Dataset.from_tensor_slices({\"x\": data[\"X_test\"], \"y\": data[\"y_test\"]}).batch(self.config[\"batch_size\"])\n",
        "            federated_train_data.append(train_ds)\n",
        "            federated_test_data.append(test_ds)\n",
        "        return federated_train_data, federated_test_data\n",
        "\n",
        "# åŸ·è¡Œè³‡æ–™è™•ç†\n",
        "processor = FederatedDataProcessor(FEDERATED_CONFIG)\n",
        "client_data, feature_names = processor.load_and_split_data()\n",
        "federated_train_data, federated_test_data = processor.create_tf_datasets(client_data)\n",
        "\n",
        "print(\"âœ… è¯é‚¦è³‡æ–™é›†æº–å‚™å®Œæˆ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hVRADm_Ordyl"
      },
      "outputs": [],
      "source": [
        "# @title Cell 3ï½œè‡ªå¯¦ç¾é«˜éšè¯é‚¦å­¸ç¿’æ¡†æ¶ï¼ˆé¿å…TFFç›¸ä¾æ€§ï¼‰\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import copy\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# =========================================================\n",
        "# â¶ è¯é‚¦å­¸ç¿’æ ¸å¿ƒé¡åˆ¥\n",
        "# =========================================================\n",
        "class FederatedSliceModel:\n",
        "    \"\"\"é«˜éšè¯é‚¦åˆ‡ç‰‡æ¨¡å‹ï¼Œæ”¯æ´å€‹äººåŒ–èˆ‡å·®åˆ†éš±ç§\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, config: Dict):\n",
        "        self.input_dim = input_dim\n",
        "        self.config = config\n",
        "\n",
        "    def create_model(self) -> tf.keras.Model:\n",
        "        \"\"\"å‰µå»ºæ”¯æ´å€‹äººåŒ–çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹\"\"\"\n",
        "        inputs = tf.keras.layers.Input(shape=(self.input_dim,))\n",
        "\n",
        "        # å…±äº«å±¤ï¼ˆåƒèˆ‡èšåˆï¼‰\n",
        "        x = tf.keras.layers.Dense(256, activation='relu', name='shared_dense_1')(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization(name='shared_bn_1')(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(128, activation='relu', name='shared_dense_2')(x)\n",
        "        x = tf.keras.layers.BatchNormalization(name='shared_bn_2')(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "        # å€‹äººåŒ–å±¤ï¼ˆä¸åƒèˆ‡èšåˆï¼‰\n",
        "        if self.config.get(\"personalization\", False):\n",
        "            personal_branch = tf.keras.layers.Dense(64, activation='relu',\n",
        "                                                   name='personal_layer')(x)\n",
        "            personal_branch = tf.keras.layers.Dropout(0.2)(personal_branch)\n",
        "            x = tf.keras.layers.Concatenate()([x, personal_branch])\n",
        "\n",
        "        x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "        outputs = tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        # ç·¨è­¯æ¨¡å‹\n",
        "        optimizer = tf.keras.optimizers.Adam(self.config[\"learning_rate\"])\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "        return model\n",
        "\n",
        "class DifferentialPrivacyOptimizer:\n",
        "    \"\"\"å·®åˆ†éš±ç§å„ªåŒ–å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, noise_multiplier: float, l2_norm_clip: float):\n",
        "        self.noise_multiplier = noise_multiplier\n",
        "        self.l2_norm_clip = l2_norm_clip\n",
        "\n",
        "    def add_noise_to_gradients(self, gradients: List[tf.Tensor]) -> List[tf.Tensor]:\n",
        "        \"\"\"ç‚ºæ¢¯åº¦æ·»åŠ é«˜æ–¯å™ªè²\"\"\"\n",
        "        noisy_gradients = []\n",
        "\n",
        "        for grad in gradients:\n",
        "            if grad is not None:\n",
        "                # æ¢¯åº¦è£å‰ª\n",
        "                clipped_grad = tf.clip_by_norm(grad, self.l2_norm_clip)\n",
        "\n",
        "                # æ·»åŠ é«˜æ–¯å™ªè²\n",
        "                noise = tf.random.normal(\n",
        "                    shape=tf.shape(clipped_grad),\n",
        "                    mean=0.0,\n",
        "                    stddev=self.noise_multiplier * self.l2_norm_clip,\n",
        "                    dtype=clipped_grad.dtype\n",
        "                )\n",
        "\n",
        "                noisy_grad = clipped_grad + noise\n",
        "                noisy_gradients.append(noisy_grad)\n",
        "            else:\n",
        "                noisy_gradients.append(None)\n",
        "\n",
        "        return noisy_gradients\n",
        "\n",
        "class SecureAggregator:\n",
        "    \"\"\"å®‰å…¨èšåˆå™¨ï¼Œæ¨¡æ“¬åŒæ…‹åŠ å¯†èˆ‡å®‰å…¨å¤šæ–¹è¨ˆç®—\"\"\"\n",
        "\n",
        "    def __init__(self, num_clients: int):\n",
        "        self.num_clients = num_clients\n",
        "        self.client_secrets = {}\n",
        "\n",
        "    def generate_client_secrets(self):\n",
        "        \"\"\"ç‚ºæ¯å€‹å®¢æˆ¶ç«¯ç”Ÿæˆå¯†é‘°\"\"\"\n",
        "        for i in range(self.num_clients):\n",
        "            self.client_secrets[i] = np.random.randint(0, 1000000, size=1)[0]\n",
        "\n",
        "    def encrypt_weights(self, weights: List[np.ndarray], client_id: int) -> List[np.ndarray]:\n",
        "        \"\"\"æ¨¡æ“¬æ¬Šé‡åŠ å¯†ï¼ˆç°¡åŒ–ç‰ˆåŒæ…‹åŠ å¯†ï¼‰\"\"\"\n",
        "        secret = self.client_secrets.get(client_id, 0)\n",
        "        encrypted_weights = []\n",
        "\n",
        "        for w in weights:\n",
        "            # ç°¡åŒ–çš„åŠ æ³•åŒæ…‹åŠ å¯†æ¨¡æ“¬\n",
        "            noise = np.random.normal(0, 0.001, w.shape) * secret / 1000000\n",
        "            encrypted_w = w + noise\n",
        "            encrypted_weights.append(encrypted_w)\n",
        "\n",
        "        return encrypted_weights\n",
        "\n",
        "    def secure_aggregate(self, encrypted_weights_list: List[List[np.ndarray]]) -> List[np.ndarray]:\n",
        "        \"\"\"å®‰å…¨èšåˆå¤šå€‹å®¢æˆ¶ç«¯çš„åŠ å¯†æ¬Šé‡\"\"\"\n",
        "        if not encrypted_weights_list:\n",
        "            return []\n",
        "\n",
        "        # åˆå§‹åŒ–èšåˆçµæœ\n",
        "        aggregated_weights = []\n",
        "        num_layers = len(encrypted_weights_list[0])\n",
        "\n",
        "        for layer_idx in range(num_layers):\n",
        "            layer_weights = [client_weights[layer_idx] for client_weights in encrypted_weights_list]\n",
        "\n",
        "            # åŠ æ¬Šå¹³å‡ï¼ˆæ¨¡æ“¬å®‰å…¨èšåˆï¼‰\n",
        "            aggregated_layer = np.mean(layer_weights, axis=0)\n",
        "            aggregated_weights.append(aggregated_layer)\n",
        "\n",
        "        return aggregated_weights\n",
        "\n",
        "class AdvancedFederatedLearning:\n",
        "    \"\"\"é«˜éšè¯é‚¦å­¸ç¿’ç³»çµ±\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.global_model = None\n",
        "        self.client_models = {}\n",
        "        self.training_history = {\n",
        "            \"rounds\": [],\n",
        "            \"global_loss\": [],\n",
        "            \"client_losses\": [],\n",
        "            \"privacy_budget\": [],\n",
        "            \"communication_cost\": [],\n",
        "            \"convergence_rate\": []\n",
        "        }\n",
        "\n",
        "        # åˆå§‹åŒ–çµ„ä»¶\n",
        "        if config.get(\"dp_enabled\", False):\n",
        "            self.dp_optimizer = DifferentialPrivacyOptimizer(\n",
        "                config[\"dp_noise_multiplier\"],\n",
        "                config[\"dp_l2_norm_clip\"]\n",
        "            )\n",
        "        else:\n",
        "            self.dp_optimizer = None\n",
        "\n",
        "        if config.get(\"secure_aggregation\", False):\n",
        "            self.secure_aggregator = SecureAggregator(config[\"num_clients\"])\n",
        "            self.secure_aggregator.generate_client_secrets()\n",
        "        else:\n",
        "            self.secure_aggregator = None\n",
        "\n",
        "    def initialize_global_model(self, input_dim: int):\n",
        "        \"\"\"åˆå§‹åŒ–å…¨åŸŸæ¨¡å‹\"\"\"\n",
        "        model_builder = FederatedSliceModel(input_dim, self.config)\n",
        "        self.global_model = model_builder.create_model()\n",
        "        print(f\"âœ… å…¨åŸŸæ¨¡å‹åˆå§‹åŒ–å®Œæˆ - åƒæ•¸æ•¸é‡: {self.global_model.count_params():,}\")\n",
        "\n",
        "    def create_client_data(self, df: pd.DataFrame, feature_names: List[str]) -> Dict:\n",
        "        \"\"\"å‰µå»ºå®¢æˆ¶ç«¯è³‡æ–™åˆ†å‰²ï¼ˆNon-IIDï¼‰\"\"\"\n",
        "        print(\"ğŸ”„ å‰µå»ºNon-IIDè¯é‚¦è³‡æ–™åˆ†å‰²...\")\n",
        "\n",
        "        client_data = {}\n",
        "\n",
        "        for i, bs_id in enumerate(self.config[\"base_stations\"]):\n",
        "            # æŒ‰åŸºç«™åˆ†å‰²è³‡æ–™\n",
        "            client_df = df[df[\"bs_id\"] == bs_id].copy()\n",
        "\n",
        "            if len(client_df) == 0:\n",
        "                print(f\"âš ï¸ åŸºç«™ {bs_id} ç„¡è³‡æ–™\")\n",
        "                continue\n",
        "\n",
        "            # å‰µå»ºNon-IIDç‰¹æ€§ï¼šä¸åŒåŸºç«™å°ˆæ³¨ä¸åŒåˆ‡ç‰‡é¡å‹\n",
        "            slice_preference = i % 3  # 0: eMBB, 1: URLLC, 2: mMTC\n",
        "\n",
        "            # 80% åå¥½åˆ‡ç‰‡ + 20% å…¶ä»–åˆ‡ç‰‡\n",
        "            if 'slice_id' in client_df.columns:\n",
        "                preferred_data = client_df[client_df['slice_id'] == slice_preference]\n",
        "                other_data = client_df[client_df['slice_id'] != slice_preference]\n",
        "\n",
        "                if len(preferred_data) > 0 and len(other_data) > 0:\n",
        "                    n_preferred = min(len(preferred_data), int(0.8 * min(50000, len(client_df))))\n",
        "                    n_other = min(len(other_data), int(0.2 * min(50000, len(client_df))))\n",
        "\n",
        "                    selected_data = pd.concat([\n",
        "                        preferred_data.sample(n=n_preferred, random_state=42+i),\n",
        "                        other_data.sample(n=n_other, random_state=42+i)\n",
        "                    ])\n",
        "                else:\n",
        "                    selected_data = client_df.sample(n=min(50000, len(client_df)), random_state=42+i)\n",
        "            else:\n",
        "                selected_data = client_df.sample(n=min(50000, len(client_df)), random_state=42+i)\n",
        "\n",
        "            # æå–ç‰¹å¾µå’Œç›®æ¨™\n",
        "            X = selected_data[feature_names].astype(np.float32).values\n",
        "            y = selected_data[\"allocation_efficiency\"].astype(np.float32).values\n",
        "\n",
        "            # æ¸…ç†è³‡æ–™\n",
        "            mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
        "            X, y = X[mask], y[mask]\n",
        "\n",
        "            if len(X) < 100:\n",
        "                print(f\"âš ï¸ å®¢æˆ¶ç«¯ {i} è³‡æ–™ä¸è¶³\")\n",
        "                continue\n",
        "\n",
        "            # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # æ¨™æº–åŒ–\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
        "            X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "            client_data[f\"client_{i}\"] = {\n",
        "                \"client_id\": i,\n",
        "                \"bs_id\": bs_id,\n",
        "                \"slice_preference\": slice_preference,\n",
        "                \"X_train\": X_train_scaled,\n",
        "                \"y_train\": y_train,\n",
        "                \"X_test\": X_test_scaled,\n",
        "                \"y_test\": y_test,\n",
        "                \"scaler\": scaler,\n",
        "                \"data_size\": len(X_train)\n",
        "            }\n",
        "\n",
        "            print(f\"ğŸ“Š å®¢æˆ¶ç«¯ {i} (BS-{bs_id}): {len(X_train):,} è¨“ç·´, {len(X_test):,} æ¸¬è©¦, åå¥½åˆ‡ç‰‡={slice_preference}\")\n",
        "\n",
        "        return client_data\n",
        "\n",
        "    def federated_averaging(self, client_weights_list: List[List[np.ndarray]],\n",
        "                          client_sizes: List[int]) -> List[np.ndarray]:\n",
        "        \"\"\"FedAvgç®—æ³•å¯¦ç¾\"\"\"\n",
        "        if not client_weights_list:\n",
        "            return []\n",
        "\n",
        "        total_size = sum(client_sizes)\n",
        "        averaged_weights = []\n",
        "\n",
        "        # è¨ˆç®—åŠ æ¬Šå¹³å‡\n",
        "        for layer_idx in range(len(client_weights_list[0])):\n",
        "            weighted_sum = None\n",
        "\n",
        "            for client_weights, size in zip(client_weights_list, client_sizes):\n",
        "                layer_weight = client_weights[layer_idx] * (size / total_size)\n",
        "\n",
        "                if weighted_sum is None:\n",
        "                    weighted_sum = layer_weight\n",
        "                else:\n",
        "                    weighted_sum += layer_weight\n",
        "\n",
        "            averaged_weights.append(weighted_sum)\n",
        "\n",
        "        return averaged_weights\n",
        "\n",
        "    def train_client(self, client_id: str, client_data: Dict) -> Tuple[List[np.ndarray], Dict]:\n",
        "        \"\"\"è¨“ç·´å–®ä¸€å®¢æˆ¶ç«¯\"\"\"\n",
        "        # å‰µå»ºå®¢æˆ¶ç«¯æ¨¡å‹\n",
        "        model_builder = FederatedSliceModel(client_data[\"X_train\"].shape[1], self.config)\n",
        "        client_model = model_builder.create_model()\n",
        "\n",
        "        # å¾å…¨åŸŸæ¨¡å‹è¼‰å…¥æ¬Šé‡\n",
        "        if self.global_model is not None:\n",
        "            client_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "        # æœ¬åœ°è¨“ç·´\n",
        "        history = client_model.fit(\n",
        "            client_data[\"X_train\"],\n",
        "            client_data[\"y_train\"],\n",
        "            validation_data=(client_data[\"X_test\"], client_data[\"y_test\"]),\n",
        "            epochs=self.config[\"local_epochs\"],\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # ç²å–è¨“ç·´å¾Œçš„æ¬Šé‡\n",
        "        trained_weights = client_model.get_weights()\n",
        "\n",
        "        # æ‡‰ç”¨å·®åˆ†éš±ç§ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
        "        if self.dp_optimizer is not None:\n",
        "            # è¨ˆç®—æ¢¯åº¦ä¸¦æ·»åŠ å™ªè²\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = client_model(client_data[\"X_train\"], training=True)\n",
        "                loss = tf.keras.losses.MeanSquaredError()(client_data[\"y_train\"], predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, client_model.trainable_variables)\n",
        "            noisy_gradients = self.dp_optimizer.add_noise_to_gradients(gradients)\n",
        "\n",
        "            # æ‡‰ç”¨å™ªè²æ¢¯åº¦æ›´æ–°\n",
        "            for i, (weight, grad) in enumerate(zip(trained_weights, noisy_gradients)):\n",
        "                if grad is not None:\n",
        "                    trained_weights[i] = weight - self.config[\"learning_rate\"] * grad.numpy()\n",
        "\n",
        "        return trained_weights, {\n",
        "            \"loss\": history.history[\"loss\"][-1],\n",
        "            \"val_loss\": history.history[\"val_loss\"][-1],\n",
        "            \"data_size\": client_data[\"data_size\"]\n",
        "        }\n",
        "\n",
        "    def run_federated_training(self, client_data: Dict):\n",
        "        \"\"\"åŸ·è¡Œè¯é‚¦å­¸ç¿’è¨“ç·´\"\"\"\n",
        "        print(f\"\\nğŸš€ é–‹å§‹è¯é‚¦å­¸ç¿’è¨“ç·´...\")\n",
        "        print(f\"ğŸ“Š é…ç½®: {len(client_data)} å€‹å®¢æˆ¶ç«¯, {self.config['rounds']} è¼ª\")\n",
        "\n",
        "        # åˆå§‹åŒ–å…¨åŸŸæ¨¡å‹\n",
        "        sample_client = next(iter(client_data.values()))\n",
        "        self.initialize_global_model(sample_client[\"X_train\"].shape[1])\n",
        "\n",
        "        for round_num in range(1, self.config[\"rounds\"] + 1):\n",
        "            print(f\"\\nğŸ”„ ç¬¬ {round_num}/{self.config['rounds']} è¼ª\")\n",
        "\n",
        "            # å®¢æˆ¶ç«¯é¸æ“‡\n",
        "            available_clients = list(client_data.keys())\n",
        "            if len(available_clients) > self.config.get(\"clients_per_round\", len(available_clients)):\n",
        "                selected_clients = np.random.choice(\n",
        "                    available_clients,\n",
        "                    size=self.config[\"clients_per_round\"],\n",
        "                    replace=False\n",
        "                )\n",
        "            else:\n",
        "                selected_clients = available_clients\n",
        "\n",
        "            # ä¸¦è¡Œè¨“ç·´å®¢æˆ¶ç«¯\n",
        "            client_weights_list = []\n",
        "            client_sizes = []\n",
        "            round_losses = []\n",
        "\n",
        "            for client_id in selected_clients:\n",
        "                try:\n",
        "                    weights, metrics = self.train_client(client_id, client_data[client_id])\n",
        "\n",
        "                    # å®‰å…¨èšåˆï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
        "                    if self.secure_aggregator is not None:\n",
        "                        client_idx = int(client_id.split('_')[1])\n",
        "                        weights = self.secure_aggregator.encrypt_weights(weights, client_idx)\n",
        "\n",
        "                    client_weights_list.append(weights)\n",
        "                    client_sizes.append(metrics[\"data_size\"])\n",
        "                    round_losses.append(metrics[\"loss\"])\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ å®¢æˆ¶ç«¯ {client_id} è¨“ç·´å¤±æ•—: {e}\")\n",
        "\n",
        "            # èšåˆæ¬Šé‡\n",
        "            if client_weights_list:\n",
        "                if self.secure_aggregator is not None:\n",
        "                    aggregated_weights = self.secure_aggregator.secure_aggregate(client_weights_list)\n",
        "                else:\n",
        "                    aggregated_weights = self.federated_averaging(client_weights_list, client_sizes)\n",
        "\n",
        "                # æ›´æ–°å…¨åŸŸæ¨¡å‹\n",
        "                self.global_model.set_weights(aggregated_weights)\n",
        "\n",
        "                # è©•ä¼°å…¨åŸŸæ¨¡å‹\n",
        "                global_metrics = self.evaluate_global_model(client_data)\n",
        "\n",
        "                # è¨˜éŒ„æ­·å²\n",
        "                self.training_history[\"rounds\"].append(round_num)\n",
        "                self.training_history[\"global_loss\"].append(global_metrics[\"loss\"])\n",
        "                self.training_history[\"client_losses\"].append(np.mean(round_losses))\n",
        "\n",
        "                # è¨ˆç®—éš±ç§é ç®—ï¼ˆå¦‚æœå•Ÿç”¨DPï¼‰\n",
        "                if self.dp_optimizer is not None:\n",
        "                    privacy_budget = round_num * self.config[\"dp_noise_multiplier\"]\n",
        "                    self.training_history[\"privacy_budget\"].append(privacy_budget)\n",
        "\n",
        "                # è¨ˆç®—é€šè¨Šæˆæœ¬\n",
        "                model_size = sum(np.prod(w.shape) for w in aggregated_weights)\n",
        "                comm_cost = model_size * len(selected_clients)\n",
        "                self.training_history[\"communication_cost\"].append(comm_cost)\n",
        "\n",
        "                # é€²åº¦å ±å‘Š\n",
        "                if round_num % 5 == 0:\n",
        "                    print(f\"   å…¨åŸŸæå¤±: {global_metrics['loss']:.6f}\")\n",
        "                    print(f\"   å¹³å‡å®¢æˆ¶ç«¯æå¤±: {np.mean(round_losses):.6f}\")\n",
        "                    print(f\"   åƒèˆ‡å®¢æˆ¶ç«¯: {len(selected_clients)}\")\n",
        "\n",
        "                    if self.dp_optimizer is not None:\n",
        "                        print(f\"   éš±ç§é ç®— (Îµ): {privacy_budget:.2f}\")\n",
        "\n",
        "            # æ—©åœæª¢æŸ¥\n",
        "            if round_num > 10:\n",
        "                recent_losses = self.training_history[\"global_loss\"][-5:]\n",
        "                if len(recent_losses) == 5:\n",
        "                    improvement = (recent_losses[0] - recent_losses[-1]) / recent_losses[0]\n",
        "                    if improvement < 0.001:  # æ”¹å–„å°æ–¼0.1%\n",
        "                        print(f\"ğŸ›‘ æ—©åœï¼šæ”¹å–„å¹…åº¦éå° ({improvement:.4f})\")\n",
        "                        break\n",
        "\n",
        "        print(\"âœ… è¯é‚¦å­¸ç¿’è¨“ç·´å®Œæˆï¼\")\n",
        "        return self.training_history\n",
        "\n",
        "    def evaluate_global_model(self, client_data: Dict) -> Dict:\n",
        "        \"\"\"è©•ä¼°å…¨åŸŸæ¨¡å‹åœ¨æ‰€æœ‰å®¢æˆ¶ç«¯ä¸Šçš„æ•ˆèƒ½\"\"\"\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for client_info in client_data.values():\n",
        "            if len(client_info[\"X_test\"]) > 0:\n",
        "                loss, _ = self.global_model.evaluate(\n",
        "                    client_info[\"X_test\"],\n",
        "                    client_info[\"y_test\"],\n",
        "                    verbose=0\n",
        "                )\n",
        "                samples = len(client_info[\"X_test\"])\n",
        "                total_loss += loss * samples\n",
        "                total_samples += samples\n",
        "\n",
        "        avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')\n",
        "        return {\"loss\": avg_loss}\n",
        "\n",
        "# =========================================================\n",
        "# â· åŸ·è¡Œè¯é‚¦å­¸ç¿’\n",
        "# =========================================================\n",
        "def run_advanced_federated_learning():\n",
        "    \"\"\"åŸ·è¡Œé«˜éšè¯é‚¦å­¸ç¿’ä¸»ç¨‹åº\"\"\"\n",
        "    print(\"ğŸš€ å•Ÿå‹•é«˜éšè¯é‚¦å­¸ç¿’ç³»çµ±...\")\n",
        "\n",
        "    # è¼‰å…¥è³‡æ–™\n",
        "    if os.path.exists(\"coloran_processed_features.parquet\"):\n",
        "        df = pd.read_parquet(\"coloran_processed_features.parquet\")\n",
        "        print(f\"âœ… è¼‰å…¥è³‡æ–™: {len(df):,} ç­†è¨˜éŒ„\")\n",
        "    else:\n",
        "        print(\"âŒ æ‰¾ä¸åˆ°è™•ç†å¾Œçš„ç‰¹å¾µè³‡æ–™\")\n",
        "        return None\n",
        "\n",
        "    # è¼‰å…¥ç‰¹å¾µåç¨±\n",
        "    with open(\"feature_metadata.json\", \"r\") as f:\n",
        "        feature_names = json.load(f)[\"feature_names\"]\n",
        "\n",
        "    # å‰µå»ºè¯é‚¦å­¸ç¿’ç³»çµ±\n",
        "    fed_system = AdvancedFederatedLearning(FEDERATED_CONFIG)\n",
        "\n",
        "    # å‰µå»ºå®¢æˆ¶ç«¯è³‡æ–™\n",
        "    client_data = fed_system.create_client_data(df, feature_names)\n",
        "\n",
        "    if not client_data:\n",
        "        print(\"âŒ ç„¡æ³•å‰µå»ºå®¢æˆ¶ç«¯è³‡æ–™\")\n",
        "        return None\n",
        "\n",
        "    # åŸ·è¡Œè¯é‚¦è¨“ç·´\n",
        "    history = fed_system.run_federated_training(client_data)\n",
        "\n",
        "    # ä¿å­˜çµæœ\n",
        "    results = {\n",
        "        \"config\": FEDERATED_CONFIG,\n",
        "        \"history\": history,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(\"federated_learning_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "    # ä¿å­˜æ¨¡å‹\n",
        "    fed_system.global_model.save(\"federated_global_model.h5\")\n",
        "\n",
        "    print(\"ğŸ’¾ çµæœå·²ä¿å­˜åˆ° federated_learning_results.json\")\n",
        "    print(\"ğŸ’¾ å…¨åŸŸæ¨¡å‹å·²ä¿å­˜åˆ° federated_global_model.h5\")\n",
        "\n",
        "    return fed_system, history\n",
        "\n",
        "# åŸ·è¡Œè¯é‚¦å­¸ç¿’\n",
        "if __name__ == \"__main__\":\n",
        "    federated_system, training_history = run_advanced_federated_learning()\n",
        "    print(\"ğŸ‰ é«˜éšè¯é‚¦å­¸ç¿’å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6X4Bz7Nrrgap"
      },
      "outputs": [],
      "source": [
        "# @title Cell 4ï½œA100 GPU åŠ é€Ÿæ©Ÿå™¨å­¸ç¿’è¨“ç·´å™¨ï¼ˆç¶²è·¯åˆ‡ç‰‡è³‡æºåˆ†é…å„ªåŒ–ï¼‰\n",
        "import os, json, warnings, joblib, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ========= A. å®‰è£å¿…è¦å¥—ä»¶ & GPU åµæ¸¬ =========\n",
        "def install_required_packages():\n",
        "    \"\"\"å®‰è£å¿…è¦çš„ GPU å¥—ä»¶ï¼ˆé©ç”¨æ–¼ A100ï¼‰\"\"\"\n",
        "    print(\"ğŸ”„ æª¢æŸ¥å¿…è¦å¥—ä»¶...\")\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        print(f\"âœ“ TensorFlow {tf.__version__} å·²å®‰è£\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ å®‰è£ TensorFlow...\")\n",
        "        !pip install -q tensorflow\n",
        "\n",
        "    try:\n",
        "        import cuml\n",
        "        print(f\"âœ“ cuML {cuml.__version__} å·²å®‰è£\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ å®‰è£ cuML...\")\n",
        "        # A100 ç’°å¢ƒéœ€è¦çš„ç‰¹å®šç‰ˆæœ¬\n",
        "        !pip install -q cuml-cu11 --extra-index-url=https://pypi.ngc.nvidia.com\n",
        "        # é‡æ–°å°å…¥\n",
        "        try:\n",
        "            import cuml\n",
        "            print(f\"âœ“ cuML {cuml.__version__} å®‰è£æˆåŠŸ\")\n",
        "        except ImportError:\n",
        "            print(\"âŒ cuML å®‰è£å¤±æ•—ï¼Œå°‡ä½¿ç”¨ CPU å‚™é¸æ–¹æ¡ˆ\")\n",
        "\n",
        "def setup_gpu_environment():\n",
        "    \"\"\"è¨­å®š GPU ç’°å¢ƒä¸¦æª¢æŸ¥å¯ç”¨æ€§\"\"\"\n",
        "    gpu = {\"gpu_available\": False, \"cuml_available\": False, \"tf_gpu\": False}\n",
        "\n",
        "    # æª¢æŸ¥ NVIDIA GPU\n",
        "    try:\n",
        "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            gpu[\"gpu_available\"] = True\n",
        "            print(\"âœ… NVIDIA GPU å·²åµæ¸¬åˆ°\")\n",
        "            # é¡¯ç¤º GPU è³‡è¨Š\n",
        "            for line in result.stdout.split('\\n')[:10]:\n",
        "                if \"A100\" in line:\n",
        "                    print(f\"  {line.strip()}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âš ï¸ nvidia-smi æœªæ‰¾åˆ°\")\n",
        "\n",
        "    # è¨­å®š TensorFlow\n",
        "    if gpu[\"gpu_available\"]:\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "            if gpus:\n",
        "                for g in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(g, True)\n",
        "                gpu[\"tf_gpu\"] = True\n",
        "                print(f\"âœ… TensorFlow å·²å•Ÿç”¨ {len(gpus)} å€‹ GPU\")\n",
        "                # è¨­å®š GPU è¨˜æ†¶é«”é™åˆ¶\n",
        "                tf.config.experimental.set_virtual_device_configuration(\n",
        "                    gpus[0],\n",
        "                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=32*1024)]\n",
        "                )\n",
        "                print(\"  å·²è¨­å®š GPU è¨˜æ†¶é«”é™åˆ¶ï¼š32GB\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ TensorFlow GPU åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
        "\n",
        "    # æª¢æŸ¥ cuML\n",
        "    try:\n",
        "        import cuml\n",
        "        from cuml.common import logger\n",
        "        logger.set_level(logger.level_enum.info)\n",
        "        gpu[\"cuml_available\"] = True\n",
        "        print(\"âœ… cuML å·²å•Ÿç”¨ - å°‡ä½¿ç”¨ GPU åŠ é€Ÿéš¨æ©Ÿæ£®æ—\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ cuML æœªæ‰¾åˆ° - å°‡ä½¿ç”¨ CPU éš¨æ©Ÿæ£®æ— (å¤šåŸ·è¡Œç·’)\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ cuML åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
        "\n",
        "    return gpu\n",
        "\n",
        "# å®‰è£å¿…è¦å¥—ä»¶ä¸¦è¨­å®š GPU\n",
        "install_required_packages()\n",
        "GPU_INFO = setup_gpu_environment()\n",
        "\n",
        "# ========= B. è³‡æ–™è¼‰å…¥ & æ¡æ¨£å„ªåŒ– =========\n",
        "def load_processed_features(max_rows=None):\n",
        "    \"\"\"è¼‰å…¥ç‰¹å¾µè³‡æ–™ä¸¦é€²è¡Œæ™ºæ…§å‹æ¡æ¨£\"\"\"\n",
        "    candidates = [\n",
        "        \"coloran_processed_features.parquet\",\n",
        "        \"oran_processed_features.parquet\",\n",
        "        \"coloran_complete_dataset.parquet\",\n",
        "    ]\n",
        "\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            # æª¢æŸ¥æª”æ¡ˆå¤§å°\n",
        "            file_size_gb = os.path.getsize(p) / (1024**3)\n",
        "            print(f\"ğŸ“ æ‰¾åˆ°è³‡æ–™æª”æ¡ˆ: {p} ({file_size_gb:.2f} GB)\")\n",
        "\n",
        "            # å¤§æ–¼ 1GB ä¸” max_rows æœ‰è¨­å®šæ™‚ï¼Œä½¿ç”¨æ¡æ¨£\n",
        "            if file_size_gb > 1 and max_rows:\n",
        "                # è®€å–åˆ—æ•¸\n",
        "                file_row_count = pd.read_parquet(p, columns=[]).shape[0]\n",
        "                print(f\"  ç¸½åˆ—æ•¸: {file_row_count:,}\")\n",
        "\n",
        "                if file_row_count > max_rows:\n",
        "                    # è¨ˆç®—æ¡æ¨£æ¯”ä¾‹\n",
        "                    fraction = max_rows / file_row_count\n",
        "                    print(f\"  ä½¿ç”¨æ¡æ¨£ {fraction:.2%} ä¾†è¼‰å…¥è³‡æ–™...\")\n",
        "                    df = pd.read_parquet(p, engine='pyarrow').sample(frac=fraction, random_state=42)\n",
        "                else:\n",
        "                    df = pd.read_parquet(p, engine='pyarrow')\n",
        "            else:\n",
        "                df = pd.read_parquet(p, engine='pyarrow')\n",
        "\n",
        "            # è¼‰å…¥ç‰¹å¾µåç¨±\n",
        "            try:\n",
        "                with open(\"feature_metadata.json\", \"r\") as f:\n",
        "                    meta = json.load(f)\n",
        "                feats = meta[\"feature_names\"]\n",
        "            except FileNotFoundError:\n",
        "                # ä½¿ç”¨é è¨­ç‰¹å¾µåç¨±\n",
        "                feats = ['num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
        "                        'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
        "                        'prb_utilization', 'throughput_efficiency', 'qos_score',\n",
        "                        'network_load', 'hour', 'minute', 'day_of_week']\n",
        "\n",
        "            print(f\"âœ… è¼‰å…¥å®Œæˆ: {len(df):,} åˆ— Ã— {len(feats)} ç‰¹å¾µ\")\n",
        "            return df, feats\n",
        "\n",
        "    print(\"âŒ æ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ\")\n",
        "    return None, None\n",
        "\n",
        "# ========= C. A100 æœ€ä½³åŒ–é æ¸¬å™¨ =========\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "class A100OptimizedPredictor:\n",
        "    def __init__(self, df, feats):\n",
        "        self.df, self.feats = df, feats\n",
        "        self.scaler = self.rf_model = self.nn_model = None\n",
        "\n",
        "        # A100 å„ªåŒ–åƒæ•¸\n",
        "        self.rf_batch_size = 500000  # éš¨æ©Ÿæ£®æ—æ‰¹æ¬¡å¤§å°\n",
        "        self.nn_batch_size = 4096    # A100 é©åˆçš„æ‰¹æ¬¡å¤§å°\n",
        "\n",
        "        # è‡ªå‹•æ¡æ¨£è¨­å®š\n",
        "        self.use_sampling = len(df) > 10_000_000  # è¶…é1åƒè¬åˆ—æ‰æ¡æ¨£\n",
        "        if self.use_sampling:\n",
        "            self.sample_size = min(5_000_000, len(df))  # æœ€å¤š500è¬åˆ—\n",
        "            print(f\"ğŸ”„ å¤§å‹è³‡æ–™é›† ({len(df):,} åˆ—) - å°‡ä½¿ç”¨æ¡æ¨£ {self.sample_size:,} åˆ—\")\n",
        "\n",
        "    # --- è³‡æ–™æº–å‚™ ---\n",
        "    def prepare_data(self):\n",
        "        print(\"ğŸ“Š æº–å‚™è¨“ç·´è³‡æ–™...\")\n",
        "\n",
        "        # æ™ºæ…§æ¡æ¨£\n",
        "        if self.use_sampling:\n",
        "            sample = self.df.sample(n=self.sample_size, random_state=42)\n",
        "            X = sample[self.feats].astype(np.float32).values\n",
        "            y = sample[\"allocation_efficiency\"].astype(np.float32).values\n",
        "        else:\n",
        "            X = self.df[self.feats].astype(np.float32).values\n",
        "            y = self.df[\"allocation_efficiency\"].astype(np.float32).values\n",
        "\n",
        "        # æª¢æŸ¥è³‡æ–™\n",
        "        print(f\"  ç‰¹å¾µç¯„åœ: {X.min():.4f} åˆ° {X.max():.4f}\")\n",
        "        print(f\"  ç›®æ¨™ç¯„åœ: {y.min():.4f} åˆ° {y.max():.4f}\")\n",
        "\n",
        "        # åˆ†å‰²è³‡æ–™\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=.2, random_state=42, shuffle=True\n",
        "        )\n",
        "\n",
        "        # æ¨™æº–åŒ– - ä½¿ç”¨ float32 ç¯€çœ GPU è¨˜æ†¶é«”\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train).astype(np.float32)\n",
        "        X_test = self.scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "        print(f\"âœ… è¨“ç·´é›†: {X_train.shape}, æ¸¬è©¦é›†: {X_test.shape}\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    # --- A100 æœ€ä½³åŒ–éš¨æ©Ÿæ£®æ— ---\n",
        "    def train_random_forest(self, X_train, y_train):\n",
        "        print(\"\\nğŸŒ² è¨“ç·´éš¨æ©Ÿæ£®æ—...\")\n",
        "\n",
        "        if GPU_INFO[\"cuml_available\"]:\n",
        "            print(\"  ä½¿ç”¨ cuML GPU éš¨æ©Ÿæ£®æ—\")\n",
        "            try:\n",
        "                from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "                self.rf_model = cuRF(\n",
        "                    n_estimators=300,\n",
        "                    max_depth=16,\n",
        "                    random_state=42,\n",
        "                    max_features=0.5  # A100 è¨˜æ†¶é«”å„ªåŒ–\n",
        "                )\n",
        "                self.rf_model.fit(X_train, y_train)\n",
        "                print(\"âœ… GPU éš¨æ©Ÿæ£®æ—è¨“ç·´å®Œæˆ\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ GPU éš¨æ©Ÿæ£®æ—å¤±æ•—: {e}\")\n",
        "                print(\"  é€€å›ä½¿ç”¨ CPU ç‰ˆæœ¬\")\n",
        "                GPU_INFO[\"cuml_available\"] = False\n",
        "\n",
        "        # å¦‚æœ GPU ä¸å¯ç”¨æˆ–å¤±æ•—ï¼Œä½¿ç”¨ CPU\n",
        "        if not GPU_INFO[\"cuml_available\"]:\n",
        "            print(\"  ä½¿ç”¨ CPU éš¨æ©Ÿæ£®æ— (sklearn å¤šåŸ·è¡Œç·’)\")\n",
        "            from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "            # ä½¿ç”¨å¤§é‡åŸ·è¡Œç·’ & è¨˜æ†¶é«”å„ªåŒ–åƒæ•¸\n",
        "            cpu_count = os.cpu_count() or 4\n",
        "            print(f\"  CPU åŸ·è¡Œç·’æ•¸: {cpu_count}\")\n",
        "\n",
        "            self.rf_model = RandomForestRegressor(\n",
        "                n_estimators=200,\n",
        "                max_depth=12,\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=8,\n",
        "                random_state=42,\n",
        "                n_jobs=cpu_count,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # å¦‚æœè³‡æ–™é›†å¤ªå¤§ï¼Œæ‰¹æ¬¡è¨“ç·´\n",
        "            if len(X_train) > self.rf_batch_size:\n",
        "                print(f\"  ä½¿ç”¨æ‰¹æ¬¡è¨“ç·´: {self.rf_batch_size:,} åˆ—/æ‰¹æ¬¡\")\n",
        "                indices = np.arange(len(X_train))\n",
        "                np.random.shuffle(indices)\n",
        "                batch_indices = indices[:self.rf_batch_size]\n",
        "                self.rf_model.fit(X_train[batch_indices], y_train[batch_indices])\n",
        "            else:\n",
        "                self.rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # ç”Ÿæˆç‰¹å¾µé‡è¦æ€§\n",
        "        if hasattr(self.rf_model, \"feature_importances_\"):\n",
        "            fi = pd.DataFrame({\n",
        "                \"feature\": self.feats,\n",
        "                \"importance\": self.rf_model.feature_importances_\n",
        "            }).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "            # å„²å­˜ç‰¹å¾µé‡è¦æ€§\n",
        "            fi.to_csv(\"feature_importance.csv\", index=False)\n",
        "            print(\"ğŸ’¾ feature_importance.csv å·²å„²å­˜\")\n",
        "\n",
        "            # é¡¯ç¤ºå‰ 5 å€‹é‡è¦ç‰¹å¾µ\n",
        "            print(\"\\nğŸ“Š ç‰¹å¾µé‡è¦æ€§ Top-5:\")\n",
        "            for i, row in fi.head(5).iterrows():\n",
        "                print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "            return fi\n",
        "        return None\n",
        "\n",
        "    # --- A100 æœ€ä½³åŒ–ç¥ç¶“ç¶²è·¯ ---\n",
        "    def train_neural_net(self, X_train, y_train, X_test, y_test):\n",
        "        print(\"\\nğŸ§  è¨“ç·´ç¥ç¶“ç¶²è·¯...\")\n",
        "\n",
        "        import tensorflow as tf\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦æœ‰ GPU\n",
        "        if GPU_INFO[\"tf_gpu\"]:\n",
        "            print(\"  ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´\")\n",
        "\n",
        "            # å•Ÿç”¨æ··åˆç²¾åº¦ (æå‡ A100 æ•ˆèƒ½)\n",
        "            try:\n",
        "                from tensorflow.keras import mixed_precision\n",
        "                policy = mixed_precision.Policy('mixed_float16')\n",
        "                mixed_precision.set_global_policy(policy)\n",
        "                print(\"  å·²å•Ÿç”¨æ··åˆç²¾åº¦ (mixed_float16)\")\n",
        "            except:\n",
        "                print(\"  æœªèƒ½å•Ÿç”¨æ··åˆç²¾åº¦\")\n",
        "        else:\n",
        "            print(\"  ä½¿ç”¨ CPU è¨“ç·´ (è¼ƒæ…¢)\")\n",
        "\n",
        "        # å»ºç«‹æ¨¡å‹\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu', input_shape=(len(self.feats),)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            # è¼¸å‡ºå±¤ä½¿ç”¨ float32 é¿å…æ··åˆç²¾åº¦å•é¡Œ\n",
        "            tf.keras.layers.Dense(1, activation='linear', dtype='float32')\n",
        "        ])\n",
        "\n",
        "        # ç·¨è­¯\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        # å›èª¿å‡½æ•¸\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # è¨“ç·´\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=100,\n",
        "            batch_size=self.nn_batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.nn_model = model\n",
        "        return history\n",
        "\n",
        "    # --- è©•ä¼° ---\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        print(\"\\nğŸ“ˆ è©•ä¼°æ¨¡å‹æ•ˆèƒ½...\")\n",
        "        results = {}\n",
        "\n",
        "        if self.rf_model is not None:\n",
        "            # æ‰¹æ¬¡é æ¸¬ä»¥é¿å… OOM\n",
        "            if len(X_test) > 1000000:\n",
        "                print(\"  ä½¿ç”¨æ‰¹æ¬¡é æ¸¬è©•ä¼°éš¨æ©Ÿæ£®æ—\")\n",
        "                batch_size = 500000\n",
        "                predictions = []\n",
        "                for i in range(0, len(X_test), batch_size):\n",
        "                    end = min(i + batch_size, len(X_test))\n",
        "                    batch_pred = self.rf_model.predict(X_test[i:end])\n",
        "                    predictions.extend(batch_pred)\n",
        "                p = np.array(predictions)\n",
        "            else:\n",
        "                p = self.rf_model.predict(X_test)\n",
        "\n",
        "            results[\"Random Forest\"] = {\n",
        "                \"R2\": r2_score(y_test, p),\n",
        "                \"MAE\": mean_absolute_error(y_test, p),\n",
        "                \"MSE\": mean_squared_error(y_test, p)\n",
        "            }\n",
        "\n",
        "        if self.nn_model is not None:\n",
        "            p = self.nn_model.predict(X_test, verbose=0).flatten()\n",
        "            results[\"Neural Network\"] = {\n",
        "                \"R2\": r2_score(y_test, p),\n",
        "                \"MAE\": mean_absolute_error(y_test, p),\n",
        "                \"MSE\": mean_squared_error(y_test, p)\n",
        "            }\n",
        "\n",
        "        # é¡¯ç¤ºçµæœ\n",
        "        for model_name, metrics in results.items():\n",
        "            print(f\"\\n{model_name} è©•ä¼°çµæœ:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"  {metric}: {value:.6f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# ========= D. åŸ·è¡Œç®¡é“ =========\n",
        "def run_gpu_ml_pipeline():\n",
        "    print(\"ğŸš€ å•Ÿå‹• A100 GPU æ©Ÿå™¨å­¸ç¿’ç®¡é“...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # è¼‰å…¥è³‡æ–™ (å¦‚æœè¨˜æ†¶é«”æœ‰é™åˆ¶ï¼Œè¨­å®š max_rows)\n",
        "    df, feats = load_processed_features(max_rows=5000000)\n",
        "    if df is None:\n",
        "        return None, None\n",
        "\n",
        "    # å»ºç«‹é æ¸¬å™¨\n",
        "    predictor = A100OptimizedPredictor(df, feats)\n",
        "\n",
        "    # æº–å‚™è³‡æ–™\n",
        "    Xtr, Xte, ytr, yte = predictor.prepare_data()\n",
        "\n",
        "    # è¨“ç·´éš¨æ©Ÿæ£®æ—\n",
        "    start_time = datetime.now()\n",
        "    fi = predictor.train_random_forest(Xtr, ytr)\n",
        "    rf_time = (datetime.now() - start_time).total_seconds()\n",
        "    print(f\"â±ï¸ éš¨æ©Ÿæ£®æ—è¨“ç·´æ™‚é–“: {rf_time:.1f} ç§’\")\n",
        "\n",
        "    # è¨“ç·´ç¥ç¶“ç¶²è·¯\n",
        "    start_time = datetime.now()\n",
        "    history = predictor.train_neural_net(Xtr, ytr, Xte, yte)\n",
        "    nn_time = (datetime.now() - start_time).total_seconds()\n",
        "    print(f\"â±ï¸ ç¥ç¶“ç¶²è·¯è¨“ç·´æ™‚é–“: {nn_time:.1f} ç§’\")\n",
        "\n",
        "    # è©•ä¼°æ¨¡å‹\n",
        "    results = predictor.evaluate(Xte, yte)\n",
        "\n",
        "    # å¯«å…¥ summary\n",
        "    summary = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"data_size\": len(df),\n",
        "        \"features\": len(feats),\n",
        "        \"results\": {k: {kk: float(vv) for kk, vv in v.items()}\n",
        "                   for k, v in results.items()},\n",
        "        \"training_time\": {\n",
        "            \"random_forest\": rf_time,\n",
        "            \"neural_network\": nn_time\n",
        "        },\n",
        "        \"history_loss\": [float(x) for x in history.history.get(\"loss\", [])],\n",
        "        \"history_val_loss\": [float(x) for x in history.history.get(\"val_loss\", [])]\n",
        "    }\n",
        "\n",
        "    with open(\"training_summary_fixed.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(\"ğŸ’¾ training_summary_fixed.json å·²æ›´æ–°\")\n",
        "\n",
        "    # å„²å­˜æ¨¡å‹\n",
        "    joblib.dump(predictor.rf_model, \"fixed_rf_model.pkl\")\n",
        "    joblib.dump(predictor.scaler, \"fixed_scaler.pkl\")\n",
        "    predictor.nn_model.save(\"fixed_nn_model.h5\")\n",
        "    print(\"âœ… æ¨¡å‹èˆ‡ç¸®æ”¾å™¨å·²å„²å­˜\")\n",
        "\n",
        "    # é‡‹æ”¾è¨˜æ†¶é«”\n",
        "    gc.collect()\n",
        "\n",
        "    # å°‡çµæœè¨­ç‚ºå…¨åŸŸè®Šæ•¸ä»¥ä¾›å¾ŒçºŒä½¿ç”¨\n",
        "    global predictor_global, results_global\n",
        "    predictor_global, results_global = predictor, results\n",
        "\n",
        "    return predictor, results\n",
        "\n",
        "# ç›´æ¥åŸ·è¡Œ\n",
        "if __name__ == \"__main__\":\n",
        "    predictor, results = run_gpu_ml_pipeline()\n",
        "\n",
        "print(\"ğŸ‰ Cell 4 åŸ·è¡Œå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GLa-PLpOrida"
      },
      "outputs": [],
      "source": [
        "# @title Cell 5ï½œé«˜æ•ˆå‹•æ…‹åˆ‡ç‰‡è³‡æºåˆ†é…ï¼ˆä¿®æ­£ç“¶é ¸ç‰ˆï¼‰\n",
        "import os, json, warnings, joblib, gc, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "class EfficientSliceAllocator:\n",
        "    \"\"\"é«˜æ•ˆç‡å‹•æ…‹åˆ†é…å™¨ - è§£æ±ºç“¶é ¸å•é¡Œ\"\"\"\n",
        "\n",
        "    def __init__(self, predictor, cfg, total_rbgs=17):\n",
        "        self.predictor = predictor\n",
        "        self.cfg = cfg\n",
        "        self.total_rbgs = total_rbgs\n",
        "\n",
        "        # å„ªåŒ–åƒæ•¸\n",
        "        self.max_batch_size = 1000  # æ¸›å°‘æ‰¹æ¬¡å¤§å°\n",
        "        self.timeout_s = 300  # æ¸›å°‘è¶…æ™‚æ™‚é–“\n",
        "\n",
        "        # è¼‰å…¥ç‰¹å¾µåç¨±\n",
        "        try:\n",
        "            with open(\"feature_metadata.json\", \"r\") as f:\n",
        "                self.features = json.load(f)[\"feature_names\"]\n",
        "        except:\n",
        "            self.features = ['num_ues','slice_id','sched_policy_num','allocated_rbgs',\n",
        "                           'bs_id','exp_id','sum_requested_prbs','sum_granted_prbs',\n",
        "                           'prb_utilization','throughput_efficiency','qos_score',\n",
        "                           'network_load','hour','minute','day_of_week']\n",
        "\n",
        "        print(f\"ğŸš€ é«˜æ•ˆåˆ†é…å™¨åˆå§‹åŒ–å®Œæˆ - {len(self.features)} ç‰¹å¾µ\")\n",
        "\n",
        "    def _predict_single_allocation(self, state, allocation):\n",
        "        \"\"\"é æ¸¬å–®ä¸€åˆ†é…æ–¹æ¡ˆçš„æ•ˆç‡ - ç°¡åŒ–ç‰ˆæœ¬\"\"\"\n",
        "        # åªè¨ˆç®—åŠ æ¬Šå¹³å‡æ•ˆç‡ï¼Œä¸åˆ†åˆ¥é æ¸¬æ¯å€‹åˆ‡ç‰‡\n",
        "        feature_vector = [\n",
        "            state['num_ues'],\n",
        "            1,  # ä½¿ç”¨å¹³å‡åˆ‡ç‰‡ID\n",
        "            state['sched_policy'],\n",
        "            np.mean(allocation),  # å¹³å‡åˆ†é…\n",
        "            state['bs_id'],\n",
        "            state['exp_id'],\n",
        "            state['requested_prbs'],\n",
        "            np.sum(allocation),\n",
        "            min(1.0, state['requested_prbs'] / max(1, np.sum(allocation))),\n",
        "            state['dl_cqi'] / 15.0,\n",
        "            (state['dl_cqi'] / 15.0 + state['ul_sinr'] / 30.0) / 2.0,\n",
        "            state['num_ues'] / 42.0,\n",
        "            state['hour'],\n",
        "            state['minute'],\n",
        "            state['day_of_week']\n",
        "        ]\n",
        "\n",
        "        # å–®æ¬¡é æ¸¬\n",
        "        X = np.array([feature_vector], dtype=np.float32)\n",
        "        X_scaled = self.predictor.scaler.transform(X)\n",
        "        prediction = self.predictor.rf_model.predict(X_scaled)[0]\n",
        "\n",
        "        # åŠ å…¥åˆ†é…å¹³è¡¡åº¦çå‹µ\n",
        "        balance_penalty = np.std(allocation) * 0.01  # æ‡²ç½°ä¸å¹³è¡¡åˆ†é…\n",
        "        return np.clip(prediction - balance_penalty, 0.0, 1.0)\n",
        "\n",
        "    def optimized_exhaustive(self, state):\n",
        "        \"\"\"å„ªåŒ–çš„çª®èˆ‰æœç´¢ - æ¸›å°‘æœç´¢ç©ºé–“\"\"\"\n",
        "        best_alloc = [6, 6, 5]  # é è¨­åˆ†é…\n",
        "        best_eff = self._predict_single_allocation(state, best_alloc)\n",
        "\n",
        "        # é™åˆ¶æœç´¢ç¯„åœ - åªæœç´¢åˆç†çš„åˆ†é…\n",
        "        search_space = [\n",
        "            [4, 8, 5], [5, 7, 5], [6, 6, 5], [7, 5, 5], [8, 4, 5],\n",
        "            [5, 6, 6], [6, 5, 6], [7, 4, 6], [4, 7, 6], [5, 5, 7],\n",
        "            [6, 4, 7], [4, 6, 7], [3, 8, 6], [8, 3, 6], [9, 4, 4]\n",
        "        ]\n",
        "\n",
        "        count = 0\n",
        "        for alloc in search_space:\n",
        "            if sum(alloc) == self.total_rbgs:\n",
        "                eff = self._predict_single_allocation(state, alloc)\n",
        "                if eff > best_eff:\n",
        "                    best_eff = eff\n",
        "                    best_alloc = alloc\n",
        "                count += 1\n",
        "\n",
        "        return best_alloc, best_eff\n",
        "\n",
        "    def optimized_genetic(self, state, pop_size=20, generations=8):\n",
        "        \"\"\"å„ªåŒ–çš„éºå‚³æ¼”ç®—æ³• - æ¸›å°‘è¨ˆç®—é‡\"\"\"\n",
        "        population = [self._rand_alloc() for _ in range(pop_size)]\n",
        "\n",
        "        for gen in range(generations):\n",
        "            # è¨ˆç®—é©æ‡‰åº¦\n",
        "            fitness_scores = []\n",
        "            for alloc in population:\n",
        "                fitness_scores.append(self._predict_single_allocation(state, alloc))\n",
        "\n",
        "            fitness_scores = np.array(fitness_scores)\n",
        "\n",
        "            # é¸æ“‡æœ€å¥½çš„50%ä½œç‚ºçˆ¶æ¯\n",
        "            sorted_indices = np.argsort(fitness_scores)[::-1]\n",
        "            elite_size = pop_size // 2\n",
        "            elite_population = [population[i] for i in sorted_indices[:elite_size]]\n",
        "\n",
        "            # ç”Ÿæˆæ–°æ—ç¾¤\n",
        "            new_population = elite_population.copy()  # ä¿ç•™ç²¾è‹±\n",
        "\n",
        "            # å¡«å……å‰©é¤˜ä½ç½®\n",
        "            while len(new_population) < pop_size:\n",
        "                if len(elite_population) >= 2:\n",
        "                    p1, p2 = np.random.choice(elite_population, 2, replace=False).tolist()\n",
        "                    child = self._crossover(p1, p2)\n",
        "                    if np.random.rand() < 0.2:  # 20%è®Šç•°ç‡\n",
        "                        child = self._mutate(child)\n",
        "                    new_population.append(child)\n",
        "                else:\n",
        "                    new_population.append(self._rand_alloc())\n",
        "\n",
        "            population = new_population\n",
        "\n",
        "        # æ‰¾åˆ°æœ€ä½³è§£\n",
        "        final_fitness = [self._predict_single_allocation(state, alloc) for alloc in population]\n",
        "        best_idx = np.argmax(final_fitness)\n",
        "        return population[best_idx], final_fitness[best_idx]\n",
        "\n",
        "    def _rand_alloc(self):\n",
        "        \"\"\"ç”Ÿæˆéš¨æ©Ÿåˆ†é…\"\"\"\n",
        "        alloc = [1, 1, 1]\n",
        "        remaining = self.total_rbgs - 3\n",
        "        for _ in range(remaining):\n",
        "            alloc[np.random.randint(3)] += 1\n",
        "        return alloc\n",
        "\n",
        "    def _crossover(self, p1, p2):\n",
        "        \"\"\"äº¤å‰æ“ä½œ\"\"\"\n",
        "        cut_point = np.random.randint(1, 3)\n",
        "        child = p1[:cut_point] + p2[cut_point:]\n",
        "        return self._repair_allocation(child)\n",
        "\n",
        "    def _mutate(self, alloc):\n",
        "        \"\"\"è®Šç•°æ“ä½œ\"\"\"\n",
        "        alloc = alloc.copy()\n",
        "        i, j = np.random.choice(3, 2, replace=False)\n",
        "        if alloc[i] > 1:\n",
        "            alloc[i] -= 1\n",
        "            alloc[j] += 1\n",
        "        return alloc\n",
        "\n",
        "    def _repair_allocation(self, alloc):\n",
        "        \"\"\"ä¿®å¾©åˆ†é…\"\"\"\n",
        "        alloc = [max(1, int(x)) for x in alloc]\n",
        "        diff = self.total_rbgs - sum(alloc)\n",
        "        if diff != 0:\n",
        "            alloc[0] += diff  # èª¿æ•´ç¬¬ä¸€å€‹åˆ‡ç‰‡\n",
        "        return [max(1, x) for x in alloc]\n",
        "\n",
        "    def simulate(self, steps=50, method=\"genetic\"):\n",
        "        \"\"\"åŸ·è¡Œæ¨¡æ“¬\"\"\"\n",
        "        optimizer_fn = self.optimized_genetic if method == \"genetic\" else self.optimized_exhaustive\n",
        "        records = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"ğŸ¯ é–‹å§‹ {method} æ¨¡æ“¬ ({steps} æ­¥)...\")\n",
        "\n",
        "        for t in range(steps):\n",
        "            if time.time() - start_time > self.timeout_s:\n",
        "                print(f\"â° è¶…æ™‚åœæ­¢ï¼Œå·²å®Œæˆ {t}/{steps} æ­¥\")\n",
        "                break\n",
        "\n",
        "            # ç”Ÿæˆç¶²è·¯ç‹€æ…‹\n",
        "            current_state = {\n",
        "                'num_ues': np.random.randint(5, 25),\n",
        "                'sched_policy': np.random.randint(3),\n",
        "                'requested_prbs': np.random.randint(8, 20),\n",
        "                'dl_cqi': np.random.uniform(7, 13),\n",
        "                'ul_sinr': np.random.uniform(10, 25),\n",
        "                'hour': (t // 4) % 24,\n",
        "                'minute': (t % 4) * 15,\n",
        "                'day_of_week': t % 7,\n",
        "                'bs_id': np.random.choice([1, 8, 15, 22, 29, 36, 43]),\n",
        "                'exp_id': np.random.randint(1, 8)\n",
        "            }\n",
        "\n",
        "            # åŸºæº–åˆ†é…\n",
        "            baseline_alloc = [6, 6, 5]\n",
        "            baseline_eff = self._predict_single_allocation(current_state, baseline_alloc)\n",
        "\n",
        "            # å„ªåŒ–åˆ†é…\n",
        "            optimal_alloc, optimal_eff = optimizer_fn(current_state)\n",
        "\n",
        "            improvement = optimal_eff - baseline_eff\n",
        "            records.append({\n",
        "                'step': t,\n",
        "                'improve': improvement,\n",
        "                'alloc': optimal_alloc,\n",
        "                'baseline_eff': baseline_eff,\n",
        "                'optimal_eff': optimal_eff\n",
        "            })\n",
        "\n",
        "            # é€²åº¦å ±å‘Š\n",
        "            if (t + 1) % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                avg_improvement = np.mean([r['improve'] for r in records])\n",
        "                print(f\"   æ­¥é©Ÿ {t+1}/{steps} | è€—æ™‚: {elapsed:.1f}s | å¹³å‡æ”¹å–„: {avg_improvement:.4f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        avg_improvement = np.mean([r['improve'] for r in records]) if records else 0\n",
        "        print(f\"âœ… {method} æ¨¡æ“¬å®Œæˆï¼è€—æ™‚: {total_time:.1f}s | æœ€çµ‚å¹³å‡æ”¹å–„: {avg_improvement:.4f}\")\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "# åŸ·è¡Œå„ªåŒ–ç‰ˆæœ¬\n",
        "def run_efficient_allocation():\n",
        "    \"\"\"åŸ·è¡Œé«˜æ•ˆå‹•æ…‹åˆ†é…\"\"\"\n",
        "    # è¼‰å…¥æ¨¡å‹\n",
        "    if \"predictor\" in globals() and globals()[\"predictor\"] is not None:\n",
        "        pred = globals()[\"predictor\"]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"è«‹å…ˆåŸ·è¡Œ Cell 4 è¨“ç·´æ¨¡å‹\")\n",
        "\n",
        "    # è¼‰å…¥é…ç½®\n",
        "    try:\n",
        "        with open(\"slice_configs.json\", \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "    except:\n",
        "        cfg = {}\n",
        "\n",
        "    # å‰µå»ºåˆ†é…å™¨\n",
        "    allocator = EfficientSliceAllocator(pred, cfg)\n",
        "\n",
        "    # åŸ·è¡Œå…©ç¨®æ–¹æ³•çš„æ¯”è¼ƒ\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    results_exhaustive = allocator.simulate(40, \"exhaustive\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    results_genetic = allocator.simulate(50, \"genetic\")\n",
        "\n",
        "    # ä¿å­˜çµæœ\n",
        "    results_exhaustive.to_csv(\"efficient_exhaustive_results.csv\", index=False)\n",
        "    results_genetic.to_csv(\"efficient_genetic_results.csv\", index=False)\n",
        "\n",
        "    print(f\"\\nğŸ“Š çµæœæ¯”è¼ƒ:\")\n",
        "    print(f\"çª®èˆ‰æ³•å¹³å‡æ”¹å–„: {results_exhaustive['improve'].mean():.6f}\")\n",
        "    print(f\"éºå‚³æ¼”ç®—æ³•å¹³å‡æ”¹å–„: {results_genetic['improve'].mean():.6f}\")\n",
        "\n",
        "    return results_exhaustive, results_genetic\n",
        "\n",
        "# åŸ·è¡Œ\n",
        "if __name__ == \"__main__\":\n",
        "    results_exh, results_gen = run_efficient_allocation()\n",
        "    print(\"ğŸ‰ é«˜æ•ˆå‹•æ…‹è³‡æºåˆ†é…å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1miMznkfrlo-"
      },
      "outputs": [],
      "source": [
        "# @title Cell 6 | è¯é‚¦å­¸ç¿’çµæœè¦–è¦ºåŒ–èˆ‡æ€§èƒ½æ¯”è¼ƒ\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "class FederatedResultsVisualizer:\n",
        "    def __init__(self, training_results, allocation_results):\n",
        "        self.training_results = training_results\n",
        "        self.allocation_results = allocation_results\n",
        "\n",
        "    def create_comprehensive_visualization(self):\n",
        "        \"\"\"å‰µå»ºè¯é‚¦å­¸ç¿’å®Œæ•´è¦–è¦ºåŒ–\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle('Federated Learning for Dynamic Slice Resource Allocation - Comprehensive Results',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. è¯é‚¦å­¸ç¿’æå¤±æ›²ç·š\n",
        "        self._plot_federated_training_loss(axes[0, 0])\n",
        "\n",
        "        # 2. é€šè¨Šæˆæœ¬åˆ†æ\n",
        "        self._plot_communication_cost(axes[0, 1])\n",
        "\n",
        "        # 3. å„åŸºç«™æ•ˆç‡æå‡æ¯”è¼ƒ\n",
        "        self._plot_bs_efficiency_comparison(axes[0, 2])\n",
        "\n",
        "        # 4. è¯é‚¦ vs é›†ä¸­å¼æ¯”è¼ƒ\n",
        "        self._plot_federated_vs_centralized(axes[1, 0])\n",
        "\n",
        "        # 5. è³‡æºåˆ†é…æ¨¡å¼åˆ†æ\n",
        "        self._plot_allocation_patterns(axes[1, 1])\n",
        "\n",
        "        # 6. éš±ç§ä¿è­·æ•ˆæœ\n",
        "        self._plot_privacy_utility_tradeoff(axes[1, 2])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('federated_comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_federated_training_loss(self, ax):\n",
        "        \"\"\"ç¹ªè£½è¯é‚¦å­¸ç¿’è¨“ç·´æå¤±\"\"\"\n",
        "        rounds = self.training_results[\"training_history\"][\"rounds\"]\n",
        "        train_loss = self.training_results[\"training_history\"][\"train_loss\"]\n",
        "        test_loss = self.training_results[\"training_history\"][\"test_loss\"]\n",
        "\n",
        "        ax.plot(rounds, train_loss, 'b-', linewidth=2, label='Federated Train Loss')\n",
        "        ax.plot(rounds, test_loss, 'r-', linewidth=2, label='Federated Test Loss')\n",
        "        ax.set_xlabel('Federated Rounds')\n",
        "        ax.set_ylabel('Loss Value')\n",
        "        ax.set_title('Federated Learning Training Progress')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def _plot_communication_cost(self, ax):\n",
        "        \"\"\"ç¹ªè£½é€šè¨Šæˆæœ¬åˆ†æ\"\"\"\n",
        "        rounds = self.training_results[\"training_history\"][\"rounds\"]\n",
        "        comm_cost = self.training_results[\"training_history\"][\"communication_cost\"]\n",
        "\n",
        "        ax.plot(rounds, np.array(comm_cost) / 1e6, 'g-', linewidth=2)\n",
        "        ax.set_xlabel('Federated Rounds')\n",
        "        ax.set_ylabel('Communication Cost (M parameters)')\n",
        "        ax.set_title('Communication Cost Over Rounds')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # æ¨™æ³¨ç¸½é€šè¨Šæˆæœ¬\n",
        "        total_cost = sum(comm_cost) / 1e9\n",
        "        ax.text(0.7, 0.9, f'Total: {total_cost:.2f}B params',\n",
        "                transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
        "\n",
        "    def _plot_bs_efficiency_comparison(self, ax):\n",
        "        \"\"\"ç¹ªè£½å„åŸºç«™æ•ˆç‡æå‡æ¯”è¼ƒ\"\"\"\n",
        "        bs_stats = self.allocation_results[\"per_bs_stats\"]\n",
        "        bs_ids = list(bs_stats.keys())\n",
        "        improvements = [stats[\"mean_improvement\"] for stats in bs_stats.values()]\n",
        "        std_errors = [stats[\"std_improvement\"] for stats in bs_stats.values()]\n",
        "\n",
        "        bars = ax.bar(bs_ids, improvements, yerr=std_errors,\n",
        "                     color='skyblue', alpha=0.8, capsize=5)\n",
        "        ax.set_xlabel('Base Station ID')\n",
        "        ax.set_ylabel('Average Efficiency Improvement')\n",
        "        ax.set_title('Per-Base Station Efficiency Gains')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
        "        for bar, improvement in zip(bars, improvements):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                   f'{improvement:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    def _plot_federated_vs_centralized(self, ax):\n",
        "        \"\"\"æ¯”è¼ƒè¯é‚¦å­¸ç¿’ vs é›†ä¸­å¼å­¸ç¿’\"\"\"\n",
        "        # æ¨¡æ“¬é›†ä¸­å¼å­¸ç¿’çµæœï¼ˆåŸºæ–¼åŸå§‹ Cell 4 çµæœï¼‰\n",
        "        try:\n",
        "            with open(\"training_summary_fixed.json\", \"r\") as f:\n",
        "                centralized_results = json.load(f)\n",
        "            centralized_mae = centralized_results[\"results\"][\"Neural Network\"][\"MAE\"]\n",
        "        except:\n",
        "            centralized_mae = 0.000832  # ä¾†è‡ªåŸå§‹çµæœ\n",
        "\n",
        "        federated_mae = self.training_results[\"final_metrics\"][\"test_mae\"]\n",
        "\n",
        "        methods = ['Centralized\\nLearning', 'Federated\\nLearning']\n",
        "        mae_values = [centralized_mae, federated_mae]\n",
        "        colors = ['orange', 'purple']\n",
        "\n",
        "        bars = ax.bar(methods, mae_values, color=colors, alpha=0.8)\n",
        "        ax.set_ylabel('Test MAE')\n",
        "        ax.set_title('Centralized vs Federated Learning Performance')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
        "        for bar, mae in zip(bars, mae_values):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                   f'{mae:.6f}', ha='center', va='bottom')\n",
        "\n",
        "        # æ¨™è¨»éš±ç§ä¿è­·å„ªå‹¢\n",
        "        ax.text(0.5, 0.8, 'âœ“ Privacy Preserved\\nâœ“ Distributed Training',\n",
        "                transform=ax.transAxes, ha='center',\n",
        "                bbox=dict(boxstyle=\"round\", facecolor='lightblue', alpha=0.7))\n",
        "\n",
        "    def _plot_allocation_patterns(self, ax):\n",
        "        \"\"\"åˆ†æè³‡æºåˆ†é…æ¨¡å¼\"\"\"\n",
        "        # æå–åˆ†é…æ¨¡å¼æ•¸æ“š\n",
        "        allocations = []\n",
        "        for result in self.allocation_results[\"federated_optimization_results\"]:\n",
        "            for bs_result in result[\"clients\"].values():\n",
        "                allocations.append(bs_result[\"optimized_allocation\"])\n",
        "\n",
        "        if allocations:\n",
        "            allocations = np.array(allocations)\n",
        "            slice_names = ['eMBB', 'URLLC', 'mMTC']\n",
        "\n",
        "            # ç¹ªè£½ç®±å‹åœ–\n",
        "            ax.boxplot([allocations[:, i] for i in range(3)],\n",
        "                      labels=slice_names, patch_artist=True,\n",
        "                      boxprops=dict(facecolor='lightcoral', alpha=0.7))\n",
        "            ax.set_ylabel('Allocated RBGs')\n",
        "            ax.set_title('Resource Allocation Patterns Across Slices')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, 'No allocation data available',\n",
        "                   transform=ax.transAxes, ha='center', va='center')\n",
        "\n",
        "    def _plot_privacy_utility_tradeoff(self, ax):\n",
        "        \"\"\"ç¹ªè£½éš±ç§-æ•ˆç”¨æ¬Šè¡¡\"\"\"\n",
        "        # æ¨¡æ“¬ä¸åŒéš±ç§åƒæ•¸ä¸‹çš„æ•ˆç”¨\n",
        "        noise_levels = [0.0, 0.1, 0.5, 1.0, 2.0]\n",
        "        utility_scores = [1.0, 0.98, 0.92, 0.85, 0.75]  # æ¨¡æ“¬æ•¸æ“š\n",
        "        privacy_scores = [0.0, 0.3, 0.6, 0.8, 0.95]     # éš±ç§ä¿è­·ç¨‹åº¦\n",
        "\n",
        "        # ç•¶å‰é…ç½®é»\n",
        "        current_noise = self.training_results[\"config\"][\"dp_noise_multiplier\"]\n",
        "        current_utility = 0.98  # åŸºæ–¼ç•¶å‰çµæœ\n",
        "        current_privacy = 0.3\n",
        "\n",
        "        ax.plot(privacy_scores, utility_scores, 'b-o', linewidth=2, markersize=6,\n",
        "               label='Privacy-Utility Curve')\n",
        "        ax.scatter(current_privacy, current_utility, color='red', s=100,\n",
        "                  label=f'Current Config (noise={current_noise})', zorder=5)\n",
        "\n",
        "        ax.set_xlabel('Privacy Protection Level')\n",
        "        ax.set_ylabel('Model Utility Score')\n",
        "        ax.set_title('Privacy-Utility Tradeoff Analysis')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # æ¨™è¨»æœ€ä½³å€åŸŸ\n",
        "        ax.axvspan(0.2, 0.4, alpha=0.2, color='green', label='Optimal Range')\n",
        "\n",
        "    def generate_comparison_report(self):\n",
        "        \"\"\"ç”Ÿæˆè¯é‚¦å­¸ç¿’ vs é›†ä¸­å¼å­¸ç¿’æ¯”è¼ƒå ±å‘Š\"\"\"\n",
        "        report = {\n",
        "            \"comparison_summary\": {\n",
        "                \"federated_learning\": {\n",
        "                    \"final_test_mae\": self.training_results[\"final_metrics\"][\"test_mae\"],\n",
        "                    \"training_rounds\": len(self.training_results[\"training_history\"][\"rounds\"]),\n",
        "                    \"total_training_time\": self.training_results[\"total_training_time\"],\n",
        "                    \"communication_cost\": self.training_results[\"total_communication_cost\"],\n",
        "                    \"privacy_preserved\": True,\n",
        "                    \"distributed_training\": True\n",
        "                },\n",
        "                \"advantages\": [\n",
        "                    \"Data privacy protection - raw data never leaves base stations\",\n",
        "                    \"Distributed training reduces computational burden on central server\",\n",
        "                    \"Better scalability for large-scale 5G networks\",\n",
        "                    \"Robustness against single point of failure\",\n",
        "                    \"Compliance with data protection regulations\"\n",
        "                ],\n",
        "                \"trade_offs\": [\n",
        "                    \"Higher communication overhead\",\n",
        "                    \"Longer training time due to coordination\",\n",
        "                    \"Potential performance degradation due to privacy mechanisms\",\n",
        "                    \"More complex implementation and debugging\"\n",
        "                ]\n",
        "            },\n",
        "            \"efficiency_improvements\": self.allocation_results[\"overall_stats\"],\n",
        "            \"per_base_station_analysis\": self.allocation_results[\"per_bs_stats\"]\n",
        "        }\n",
        "\n",
        "        with open(\"federated_vs_centralized_comparison.json\", \"w\") as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        print(\"ğŸ“Š è¯é‚¦å­¸ç¿’æ¯”è¼ƒå ±å‘Šå·²ç”Ÿæˆ\")\n",
        "        return report\n",
        "\n",
        "# åŸ·è¡Œè¦–è¦ºåŒ–\n",
        "visualizer = FederatedResultsVisualizer(training_results, analysis_results)\n",
        "visualizer.create_comprehensive_visualization()\n",
        "comparison_report = visualizer.generate_comparison_report()\n",
        "\n",
        "print(\"ğŸ‰ è¯é‚¦å­¸ç¿’å®Œæ•´åˆ†æèˆ‡è¦–è¦ºåŒ–å®Œæˆ!\")\n",
        "print(\"\\nğŸ“‹ ç”Ÿæˆæª”æ¡ˆåˆ—è¡¨:\")\n",
        "print(\"   - federated_training_results.json\")\n",
        "print(\"   - federated_allocation_results.json\")\n",
        "print(\"   - federated_vs_centralized_comparison.json\")\n",
        "print(\"   - federated_comprehensive_results.png\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}